[{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b0d8ccffb8d530bff08365a148a04ee6","permalink":"https://example.com/blog/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/","section":"blog","summary":"","tags":null,"title":"Blog","type":"book"},{"authors":null,"categories":null,"content":"The Machine Learning Reproducibility Challenge (MLRC 2023) is an unique, online conference which encourages the community to investigate the reproducibility, replicability and generalisability of published claims in top conferences in the literature. We invite submissions which investigate the recently published claims, add novel insights to them, and enable reproducible research, spanning various topics in the ML literature. Submissions must be first accepted at TMLR to be considered in the MLRC 2023 Proceedings. Please read the author guidelines and submission guidelines from TMLR to get the submission format and to understand more about the reviewing process. Please read our announcement blog post for more motivation, retrospectives and roadmap for the challenge.\nScope We invite thorough reproducibility studies, including but not limited to:\nGeneralisability of published claims: novel insights and results beyond what was presented in the original paper. We recommend you choose any paper(s) published in the 2023 calendar year from the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ICCV, CVPR, TMLR, JMLR, TACL) to run your reproducibility study on. Meta-reproducibility studies on set of related papers. Research on tools enabling reproducible research. Meta analysis on the state of reproducibility in various subfields in Machine Learning. Important Dates Challenge goes live: October 23, 2023 Submit to TMLR OpenReview: https://openreview.net/group?id=TMLR Deadline to share your intent to submit a TMLR paper to MLRC: February 16th, 2024 at the following form: https://forms.gle/JJ28rLwBSxMriyE89. This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn’t already), you should then update the same form with your paper camera ready details. We aim to announce the accepted papers by May 31st, 2024, pending decisions of all papers. Camera Ready Process After you have updated the form with your accepted TMLR paper, it will finally undergo a light AC review to verify MLRC compatibility. We will publish a proceedings booklet post announcement of all decisions. Accepted papers will be featured in our website along with 5-min companion videos. For query regarding MLRC 2023, contact us at mlrc-2023@googlegroups.com.\nKoustuv Sinha\nProgram Chair, MLRC 2023\n","date":1697932800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697932800,"objectID":"ae13e4b55676c49e92519f3f42b8e2a7","permalink":"https://example.com/call_for_papers/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/call_for_papers/","section":"call_for_papers","summary":"The Machine Learning Reproducibility Challenge (MLRC 2023) is an unique, online conference which encourages the community to investigate the reproducibility, replicability and generalisability of published claims in top conferences in the literature.","tags":null,"title":"Call for Papers","type":"book"},{"authors":null,"categories":null,"content":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.g. on GitHub, GitLab, BitBucket) and anonymize it according to our double blind guidelines. Document your code appropriately Have a README.md file which describes the exact steps to run your code. You can refer to the ML Code Completeness Checklist to write the README file and make sure your code submission is complete. See this blog post on best practices for reproducibility. Compute Resources Google Colaboratory provides free GPU backed Jupyter Notebooks Instructors can apply for Google Cloud credits for their students. If you are a company that can offer cloud computing credits, please contact reproducibility.challenge@gmail.com Suggested Readings Online Proceedings of MLRC Arvind Narayanan et al, 2023; Talk: Evaluating LLMs is a Minefield ACL 2022 Tutorial on “Towards Reproducible Machine Learning Research in Natural Language Processing” NAACL 2022 Reproducibility Track ML Reproducibility Checklist ML Code Completeness Checklist ML reproducibility tools and best practices Joelle Pineau’s Keynote talk on Reproducibility at NeurIPS 2018 ","date":1697932800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697932800,"objectID":"1db3ba364adb4d30a0f8fd5677683f69","permalink":"https://example.com/challenge_resources/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/challenge_resources/","section":"challenge_resources","summary":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.","tags":null,"title":"Resources","type":"book"},{"authors":null,"categories":null,"content":"This page consists of some frequently asked questions about the challenge.\nCan I reproduce workshop papers from the listed conferences? Unfortunately no, as of now we are not accepting reports on workshop papers from the listed conferences. Please work on any accepted papers from the conference proceedings.\nI want to reproduce paper(s) from conferences not listed in the challenge. Can I? We recommend you choose any paper(s) published in the 2023 calendar year from the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ICCV, CVPR, TMLR, JMLR, TACL) to run your reproducibility study on.\nI want to reproduce a papers from a previous year conference paper. Can I? While we recommended you to choose paper(s) to work on from the current calendar year, you can also choose to reproduce an older paper published in the same conferences.\nI am a course instructor, how do I participate officially with my course? Many thanks for your participation! You can just drop us a mail (reproducibility.challenge@gmail.com) with details of your course, and we will list it on our website!\nI am from industry, can I participate? Yes you are more than welcome to! Please consider sharing the word about the challenge to your peers in your company too!\nWhere can I get GPUs to run experiments? Check the Resources tab for more information.\nCan I contact the authors? Yes! It is highly recommended to contact the authors of the paper you are reproducing, to clarify doubts and implementation details.\nHow do I contact the authors? You can send the authors mail directly to initiate a discussion. The contact details can be found on the paper, which is linked in the pdf of the paper which is available for each paper.\nHow do I get the code of the paper? You can either search the pdf of the paper for the code, or find the link to PapersWithCode page of the paper, which is usually updated with the publicly released code of the authors.\nHow much of the code am I allowed to use? There is no restriction on the extent of the original code you can use for the reproducibility effort.\nIs the submission double blind? Yes, the report to be submitted should be double blind, according to TMLR’s submission policies. When submitting code for review, include your codebase in the Supplementary Materials, or link to an Anonymous Github URL.\nWhat is the format of the paper? Please consult TMLR’s author guidelines.\nThis is super exciting, how can I help? Thanks for your interest in our challenge! You can help out by spreading the news. If you are a course-instructor you can help by enrolling your course in the challenge. You can also sign up to be a reviewer when we share the call for reviewing for the challenge! If you are a company you can help sponsor by providing compute resources. Please contact us at reproducibility.challenge@gmail.com to list your generous offer in the Resources section.\n","date":1697932800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697932800,"objectID":"9b683c5fbff8d0d24588e34210d33cb8","permalink":"https://example.com/faq/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/faq/","section":"faq","summary":"This page consists of some frequently asked questions about the challenge.\nCan I reproduce workshop papers from the listed conferences? Unfortunately no, as of now we are not accepting reports on workshop papers from the listed conferences.","tags":null,"title":"FAQ","type":"book"},{"authors":null,"categories":null,"content":"2022 ReScience Volume 9 Issue 2\n2021 ReScience Volume 8 Issue 2\n2020 ReScience Volume 7 Issue 2\nNeurIPS 2019 ReScience Volume 6 Issue 2\nICLR 2018 ReScience Volume 5 Issue 2\n","date":1697587200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697587200,"objectID":"92a50dede0f893dc69287f93aedc7715","permalink":"https://example.com/proceedings/","publishdate":"2023-10-18T00:00:00Z","relpermalink":"/proceedings/","section":"proceedings","summary":"2022 ReScience Volume 9 Issue 2\n2021 ReScience Volume 8 Issue 2\n2020 ReScience Volume 7 Issue 2\nNeurIPS 2019 ReScience Volume 6 Issue 2\nICLR 2018 ReScience Volume 5 Issue 2","tags":null,"title":"Online Proceedings","type":"book"},{"authors":null,"categories":null,"content":"Organizing Committee MLRC 2023 Koustuv Sinha, Meta (FAIR) Jessica Zosa Forde, Brown University Mandana Samiei, McGill University, Mila Arna Ghosh, McGill University, Mila Lintang Sutawika, Eleuther AI Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Université de Montréal, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face ","date":1697932800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697932800,"objectID":"301778acff9fc802548b757f2666cc2b","permalink":"https://example.com/organizers/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/organizers/","section":"organizers","summary":"Organizing Committee MLRC 2023 Koustuv Sinha, Meta (FAIR) Jessica Zosa Forde, Brown University Mandana Samiei, McGill University, Mila Arna Ghosh, McGill University, Mila Lintang Sutawika, Eleuther AI Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Université de Montréal, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face ","tags":null,"title":"Organizers","type":"book"},{"authors":null,"categories":null,"content":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences. As we gear up to announce the seventh iteration of the challenge, MLRC 2023, we would like to share our learnings from previous years and how we plan to incorporate these lessons into the upcoming challenge.\nRetrospectives Over the years, the ML Reproducibility Challenge has been largely aimed at students beginning their journey in ML research as part of their ML coursework. This provided an accessible entry point to ML research, allowing early career researchers to participate and learn a full paper publication lifecycle - from designing the research question to investigating the limits of a scientific hypothesis to final publication acceptance. One of the success stories of this approach was from the University of Amsterdam, which designed a course around the challenge (FACT AI), and consistently produced high quality reproducibility reports in the final proceedings of the challenge. While the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nWhile the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nReproducibility is not a binary outcome The term “reproducibility” unfortunately comes with a baggage - whenever we talk about a paper to be reproducible, the expectation is this binary property - yes or no. However, the reality is way more nuanced: a paper presents multiple hypotheses (claims) of varying importance to the central claim - which some of them can be directly reproducible, others might not be; some of the claims may even be limited in terms “generalisability”. Consequently, we consistently found the quality of the reports submitted to the challenge fall into either of these two categories: a) making a sweeping claim about reproducibility, or b) diving deep and constructing a holistic view of reproducibility, replicability and generalisability of the claims presented in the original paper. Not surprisingly, the latter cohort is always highly rated by the reviewers and ends up more often in the accepted pool. From the 2020 iteration, we introduced a Reproducibility Summary template to encourage participants to focus on the central claims of the paper, and to mainly focus on this generalisability aspect - results beyond the original paper. We found that introducing this template helps the authors to focus more on these questions, thereby improving their submission.\nReproducibility is not about whether author’s code gives the same results Thanks to the continued effort made by the ML community in terms of Checklists and mandatory code submission policies, we now see \u0026gt;90% of papers accompanied by their source code. This is a very promising progress regarding reproducibility of the research in our field - the presence of code alleviates many questions and issues regarding the implementation, thereby facilitating exact reproducibility. Inadvertently, this also resulted in many MLRC submissions where authors only run the provided code and compare the numbers. While these contributions measure replicability, they are not strong research contributions which add valuable insights to the field. Instead, strong submissions tend to leverage the authors code to make exhaustive ablations, hyperparameter search and explore generalisability results on different data/models.\nRedundant reproductions of the same resource-friendly papers For several years, we find that authors tend to pick papers which are more resource-friendly - i.e. papers which can run on a single commodity GPU. This is likely a side-effect of the challenge being targeted primarily towards early career researchers. While reproducibility study on such resource-light papers is not a problem per se, it does often result in multiple reproduction reports on the same paper. We hypothesize that this is probably due to courses assigning multiple groups to work on a single paper, in order to better manage logistics. As we did not have any deduplication criteria, we explicitly inform our reviewers to not penalize multiple …","date":1697583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697583600,"objectID":"04c6a40295a0c6d20b98b1c300f0d6ae","permalink":"https://example.com/blog/announcing_mlrc2023/","publishdate":"2023-10-18T00:00:00+01:00","relpermalink":"/blog/announcing_mlrc2023/","section":"blog","summary":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences.","tags":null,"title":"Announcing MLRC 2023","type":"book"}]