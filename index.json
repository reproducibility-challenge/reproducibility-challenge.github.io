[{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b0d8ccffb8d530bff08365a148a04ee6","permalink":"https://example.com/blog/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/","section":"blog","summary":"","tags":null,"title":"Blog","type":"book"},{"authors":null,"categories":null,"content":"Call for papers announcement coming soon!\n","date":1733788800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733788800,"objectID":"ae13e4b55676c49e92519f3f42b8e2a7","permalink":"https://example.com/call_for_papers/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/call_for_papers/","section":"call_for_papers","summary":"Call for papers announcement coming soon!","tags":null,"title":"Call for Papers","type":"book"},{"authors":null,"categories":null,"content":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.g. on GitHub, GitLab, BitBucket) and anonymize it according to our double blind guidelines. Document your code appropriately Have a README.md file which describes the exact steps to run your code. You can refer to the ML Code Completeness Checklist to write the README file and make sure your code submission is complete. See this blog post on best practices for reproducibility. Compute Resources Google Colaboratory provides free GPU backed Jupyter Notebooks Instructors can apply for Google Cloud credits for their students. If you are a company that can offer cloud computing credits, please contact reproducibility.challenge@gmail.com Suggested Readings Online Proceedings of MLRC Arvind Narayanan et al, 2023; Talk: Evaluating LLMs is a Minefield ACL 2022 Tutorial on “Towards Reproducible Machine Learning Research in Natural Language Processing” NAACL 2022 Reproducibility Track ML Reproducibility Checklist ML Code Completeness Checklist ML reproducibility tools and best practices Joelle Pineau’s Keynote talk on Reproducibility at NeurIPS 2018 ","date":1697932800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697932800,"objectID":"1db3ba364adb4d30a0f8fd5677683f69","permalink":"https://example.com/challenge_resources/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/challenge_resources/","section":"challenge_resources","summary":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.","tags":null,"title":"Resources","type":"book"},{"authors":null,"categories":null,"content":"This page consists of some frequently asked questions about the challenge.\nCan I reproduce workshop papers from the listed conferences? Unfortunately no, as of now we are not accepting reports on workshop papers from the listed conferences. Please work on any accepted papers from the conference proceedings.\nI want to reproduce paper(s) from conferences not listed in the challenge. Can I? We recommend you choose any paper(s) published in the 2023 calendar year from the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ICCV, CVPR, TMLR, JMLR, TACL) to run your reproducibility study on.\nI want to reproduce a papers from a previous year conference paper. Can I? While we recommended you to choose paper(s) to work on from the current calendar year, you can also choose to reproduce an older paper published in the same conferences.\nI am a course instructor, how do I participate officially with my course? Many thanks for your participation! You can just drop us a mail (reproducibility.challenge@gmail.com) with details of your course, and we will list it on our website!\nI am from industry, can I participate? Yes you are more than welcome to! Please consider sharing the word about the challenge to your peers in your company too!\nWhere can I get GPUs to run experiments? Check the Resources tab for more information.\nCan I contact the authors? Yes! It is highly recommended to contact the authors of the paper you are reproducing, to clarify doubts and implementation details.\nHow do I contact the authors? You can send the authors mail directly to initiate a discussion. The contact details can be found on the paper, which is linked in the pdf of the paper which is available for each paper.\nHow do I get the code of the paper? You can either search the pdf of the paper for the code, or find the link to PapersWithCode page of the paper, which is usually updated with the publicly released code of the authors.\nHow much of the code am I allowed to use? There is no restriction on the extent of the original code you can use for the reproducibility effort.\nIs the submission double blind? Yes, the report to be submitted should be double blind, according to TMLR’s submission policies. When submitting code for review, include your codebase in the Supplementary Materials, or link to an Anonymous Github URL.\nWhat is the format of the paper? Please consult TMLR’s author guidelines.\nThis is super exciting, how can I help? Thanks for your interest in our challenge! You can help out by spreading the news. If you are a course-instructor you can help by enrolling your course in the challenge. You can also sign up to be a reviewer when we share the call for reviewing for the challenge! If you are a company you can help sponsor by providing compute resources. Please contact us at reproducibility.challenge@gmail.com to list your generous offer in the Resources section.\n","date":1697932800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697932800,"objectID":"9b683c5fbff8d0d24588e34210d33cb8","permalink":"https://example.com/faq/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/faq/","section":"faq","summary":"This page consists of some frequently asked questions about the challenge.\nCan I reproduce workshop papers from the listed conferences? Unfortunately no, as of now we are not accepting reports on workshop papers from the listed conferences.","tags":null,"title":"FAQ","type":"book"},{"authors":null,"categories":null,"content":"2023 🎓 Poster : NeurIPS 2024 Poster Sessions, Dec 10th to 15th, 2024, Vancouver, Canada\nGNNInterpreter: A probabilistic generative model-level explanation for Graph Neural Networks; Ana-Maria Vasilcoiu, Batu Helvacioğlu, Thies Kersten, Thijs Stessen; 🎓 Poster Explaining Temporal Graph Models through an Explorer-Navigator Framework, Miklos Hamar, Matey Krastev, Kristiyan Hristov, David Beglou Reproducibility Study of “Explaining RL Decisions with Trajectories”, Clio Feng, Colin Bot, Bart den Boef, Bart Aaldering Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported, Ethan Harvey, Mikhail Petrov, Michael C. Hughes; :mortar_board Poster Reproducibility study of FairAC, Gijs de Jong,Macha Meijer,Derck W.E. Prinzhorn,Harold Ruiter; 🎓 Poster On the Reproducibility of Post-Hoc Concept Bottleneck Models, Nesta Midavaine, Gregory Hok Tjoan Go, Diego Canez, Ioana Simion, Satchit Chatterji; 🎓 Poster Reproducibility Study of “Learning Perturbations to Explain Time Series Predictions, Jiapeng Fan, Paulius Skaigiris, Luke Cadigan, Sebastian Uriel Arias Explaining RL Decisions with Trajectories’: A Reproducibility Study, Karim Ahmed Abdel Sadek, Matteo Nulli, Joan Velja, Jort Vincenti Classwise-Shapley values for data valuation, Markus Semmler, Miguel de Benito Delgado Reproducibility Study of “ITI-GEN: Inclusive Text-to-Image Generation”, Daniel Gallo Fernández, Răzvan-Andrei Matișan, Alejandro Monroy Muñoz, Janusz Partyka; 🎓 Poster Reproducibility study of “Robust Fair Clustering: A Novel Fairness Attack and Defense Framework, Kacper Bartosik, Eren Kocadag, Vincent Loos, Lucas Ponticelli, 🎓 Poster CUDA: Curriculum of Data Augmentation for Long‐Tailed Recognition, Barath Chandran C; 🎓 Poster Reproducibility Study of “Explaining Temporal Graph Models Through an Explorer-Navigator Framework”, Christina Isaicu, Jesse Wonnink, Andreas Berentzen, Helia Ghasemi; 🎓 Poster Reproducibility Study of “Robust Fair Clustering: A Novel Fairness Attack and Defense Framework”, Iason Skylitsis, Zheng Feng, Idries Nasim, Camille Niessink; 🎓 Poster Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers, Fatemeh Nourilenjan Nokabadi, Jean-Francois Lalonde, Christian Gagné; 🎓 Poster Reproducibility study of “LICO: Explainable Models with Language-Image Consistency”, Luan Fletcher, Robert van der Klis, Martin Sedlacek, Stefan Vasilev, Christos Athanasiadis; 🎓 Poster On the Reproducibility of: “Learning Perturbations to Explain Time Series Predictions”, Wouter Bant, Ádám Divák, Jasper Eppink, Floris Six Dijkstra; 🎓 Poster Reproducibility Study: Equal Improvability: A New Fairness Notion Considering the Long-Term Impact, Berkay Chakar,Amina Izbassar,Mina Janićijević,Jakub Tomaszewski; 🎓 Poster Chain-of-Thought Unfaithfulness as Disguised Accuracy, Oliver Bentham, Nathan Stringham, Ana Marasović Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images, Shivank Garg, Manyana Tiwari Studying How to Efficiently and Effectively Guide Models with Explanations” - A Reproducibility Study, Adrian Sauter, Milan Miletić, Ryan Ott, Rohith Saai Pemmasani Prabakaran; 🎓 Poster Reproducibility Study Of Learning Fair Graph Representations Via Automated Data Augmentations, Thijmen Nijdam, Taiki Papandreou-Lazos, Jurgen de Heus, Juell Sprott; 🎓 Poster 2022 ReScience Volume 9 Issue 2\n2021 ReScience Volume 8 Issue 2\n2020 ReScience Volume 7 Issue 2\nNeurIPS 2019 ReScience Volume 6 Issue 2\nICLR 2018 ReScience Volume 5 Issue 2\n","date":1733788800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733788800,"objectID":"92a50dede0f893dc69287f93aedc7715","permalink":"https://example.com/proceedings/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/proceedings/","section":"proceedings","summary":"2023 🎓 Poster : NeurIPS 2024 Poster Sessions, Dec 10th to 15th, 2024, Vancouver, Canada\nGNNInterpreter: A probabilistic generative model-level explanation for Graph Neural Networks; Ana-Maria Vasilcoiu, Batu Helvacioğlu, Thies Kersten, Thijs Stessen; 🎓 Poster Explaining Temporal Graph Models through an Explorer-Navigator Framework, Miklos Hamar, Matey Krastev, Kristiyan Hristov, David Beglou Reproducibility Study of “Explaining RL Decisions with Trajectories”, Clio Feng, Colin Bot, Bart den Boef, Bart Aaldering Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported, Ethan Harvey, Mikhail Petrov, Michael C.","tags":null,"title":"Online Proceedings","type":"book"},{"authors":null,"categories":null,"content":"Program Chair Koustuv Sinha, Meta (FAIR) Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Université de Montréal, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face Stella Biderman, Booz Allen Hamilton, EleutherAI Arvind Narayanan, Princeton University Jesse Dodge, Allen Institute for AI ","date":1697932800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1697932800,"objectID":"301778acff9fc802548b757f2666cc2b","permalink":"https://example.com/organizers/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/organizers/","section":"organizers","summary":"Program Chair Koustuv Sinha, Meta (FAIR) Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Université de Montréal, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face Stella Biderman, Booz Allen Hamilton, EleutherAI Arvind Narayanan, Princeton University Jesse Dodge, Allen Institute for AI ","tags":null,"title":"Organizers","type":"book"},{"authors":null,"categories":null,"content":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences. As we gear up to announce the seventh iteration of the challenge, MLRC 2023, we would like to share our learnings from previous years and how we plan to incorporate these lessons into the upcoming challenge.\nRetrospectives Over the years, the ML Reproducibility Challenge has been largely aimed at students beginning their journey in ML research as part of their ML coursework. This provided an accessible entry point to ML research, allowing early career researchers to participate and learn a full paper publication lifecycle - from designing the research question to investigating the limits of a scientific hypothesis to final publication acceptance. One of the success stories of this approach was from the University of Amsterdam, which designed a course around the challenge (FACT AI), and consistently produced high quality reproducibility reports in the final proceedings of the challenge. While the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nWhile the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nReproducibility is not a binary outcome The term “reproducibility” unfortunately comes with a baggage - whenever we talk about a paper to be reproducible, the expectation is this binary property - yes or no. However, the reality is way more nuanced: a paper presents multiple hypotheses (claims) of varying importance to the central claim - which some of them can be directly reproducible, others might not be; some of the claims may even be limited in terms “generalisability”. Consequently, we consistently found the quality of the reports submitted to the challenge fall into either of these two categories: a) making a sweeping claim about reproducibility, or b) diving deep and constructing a holistic view of reproducibility, replicability and generalisability of the claims presented in the original paper. Not surprisingly, the latter cohort is always highly rated by the reviewers and ends up more often in the accepted pool. From the 2020 iteration, we introduced a Reproducibility Summary template to encourage participants to focus on the central claims of the paper, and to mainly focus on this generalisability aspect - results beyond the original paper. We found that introducing this template helps the authors to focus more on these questions, thereby improving their submission.\nReproducibility is not about whether author’s code gives the same results Thanks to the continued effort made by the ML community in terms of Checklists and mandatory code submission policies, we now see \u0026gt;90% of papers accompanied by their source code. This is a very promising progress regarding reproducibility of the research in our field - the presence of code alleviates many questions and issues regarding the implementation, thereby facilitating exact reproducibility. Inadvertently, this also resulted in many MLRC submissions where authors only run the provided code and compare the numbers. While these contributions measure replicability, they are not strong research contributions which add valuable insights to the field. Instead, strong submissions tend to leverage the authors code to make exhaustive ablations, hyperparameter search and explore generalisability results on different data/models.\nRedundant reproductions of the same resource-friendly papers For several years, we find that authors tend to pick papers which are more resource-friendly - i.e. papers which can run on a single commodity GPU. This is likely a side-effect of the challenge being targeted primarily towards early career researchers. While reproducibility study on such resource-light papers is not a problem per se, it does often result in multiple reproduction reports on the same paper. We hypothesize that this is probably due to courses assigning multiple groups to work on a single paper, in order to better manage logistics. As we did not have any deduplication criteria, we explicitly inform our reviewers to not penalize multiple …","date":1697583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697583600,"objectID":"04c6a40295a0c6d20b98b1c300f0d6ae","permalink":"https://example.com/blog/announcing_mlrc2023/","publishdate":"2023-10-18T00:00:00+01:00","relpermalink":"/blog/announcing_mlrc2023/","section":"blog","summary":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences.","tags":null,"title":"Announcing MLRC 2023","type":"book"},{"authors":null,"categories":null,"content":"Final decisions for MLRC 2023 We are now releasing the final list of decisions for MLRC 2023. This list includes the previous partial list published on July 5th, 2024. We have given additional time to TMLR to complete the reviews, however it is unfortunate that few papers are still awaiting a decision due to unresponsive Action Editors from TMLR. As we need to wrap up this edition, we are proceeding with the final list of 22 accepted papers. Congratulations to all!\nIf you are an author of the below mentioned papers and have not submitted the form with the camera ready items, please consider doing so at the earliest. We will reach out to the accepted authors soon with the next steps. We will also announce the best paper awards and share details on the logistics of NeurIPS poster session in the coming weeks.\nUpdate, Sept 13th, 2024: A couple of papers received acceptance status post our final date of MLRC 2023 acceptance. We have now incorporated them too in the final list.\nAn update on decisions July 5th, 2024\nWe initially communicated to have all decisions of MLRC 2023 out by 31st of May, 2024. Unfortunately, several submissions are still under review at TMLR, and we are waiting for the final decisions to trickle in. Overall, MLRC 2023 had 46 valid submissions, out of which we have recieved decisions on 61% of them. We are in touch with TMLR to expedite the process of decisions for the remaining submissions - we expect all decisions to come in by the next couple of weeks.\nUntil then, we are happy to announce the (partial) list of accepted papers. Congratulations to all 🎉! If you are an author of the below mentioned papers and have not submitted the form with the camera ready items, please consider doing so at the earliest. We will reach out to the accepted authors soon with the next steps.\n(partial paper list removed as we release the final list above)\n[Deprecated] Call For Papers We invite contributions from academics, practitioners and industry researchers of the ML community to submit novel and insightful reproducibility studies. Please read our blog post regarding our retrospectives of running the challenge and the future roadmap. We are happy to announce the formal partnership with Transactions of Machine Learning Research (TMLR) journal. The challenge goes live on October 23, 2023.\nWe recommend you choose any paper(s) published in the 2023 calendar year from the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ICCV, CVPR, TMLR, JMLR, TACL) to run your reproducibility study on.\nIn order for your paper to be submitted and presented at MLRC 2023, it first needs to be accepted and published at TMLR. While TMLR aims to follow a 2-months timeline to complete the review process of its regular submissions, this timeline is not guaranteed. If you haven’t already, we therefore recommend submitting your original paper to TMLR by February 16th, 2024, that is a little over 3 months in advance of the MLRC publication announcement date.\nKey Dates Challenge goes live: October 23, 2023 Deadline to share your intent to submit a TMLR paper to MLRC: February 16th, 2024 at the following form: https://forms.gle/JJ28rLwBSxMriyE89. This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn’t already), you should then update the same form with your paper camera ready details. Your accepted TMLR paper will finally undergo a light AC review to verify MLRC compatibility. We aim to announce the accepted papers by May 31st, 2024 July 17th, 2024, pending decisions of all papers. Contact Information For query regarding MLRC 2023, mail us at: mlrc-2023@googlegroups.com. For general queries, media, sponsorship, partnership requests, mail us at reproducibility.challenge@gmail.com. Organizing Committee Koustuv Sinha, Meta (FAIR) Jessica Zosa Forde, Brown University Mandana Samiei, McGill University, Mila Arna Ghosh, McGill University, Mila Lintang Sutawika, Eleuther AI Siba Smarak Panigrahi, McGill University, Mila ","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"3c5c4824ac010698afd303a3b61b9807","permalink":"https://example.com/proceedings/mlrc2023/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/proceedings/mlrc2023/","section":"proceedings","summary":"Final decisions for MLRC 2023 We are now releasing the final list of decisions for MLRC 2023. This list includes the previous partial list published on July 5th, 2024. We have given additional time to TMLR to complete the reviews, however it is unfortunate that few papers are still awaiting a decision due to unresponsive Action Editors from TMLR.","tags":null,"title":"MLRC 2023","type":"book"}]