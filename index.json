[{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b0d8ccffb8d530bff08365a148a04ee6","permalink":"https://reproml.org/blog/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/","section":"blog","summary":"","tags":null,"title":"Blog","type":"book"},{"authors":null,"categories":null,"content":" The Machine Learning Reproducibility Challenge (MLRC 2025) encourages the community to investigate the reproducibility, replicability and generalisability of published claims in top conferences in the literature.\nSubmissions must be first accepted at TMLR to be considered in the MLRC 2025 Proceedings. Please read the author guidelines and submission guidelines from TMLR to get the submission format and to understand more about the reviewing process. Existing papers related to the scope (with reproducibility certification) already published at TMLR are also welcome for the consideration of the committee. Please read our announcement blog post for more information.\nScope We invite submissions which conduct novel, unpublished research of reproducibility of machine learning methods and literature, including but not limited to :\nMethods and tools to foster reproducibility research in Machine Learning Generalisability of published claims: novel insights and results beyond what was presented in the original paper, from any paper (or set of papers) published in top ML conferences and journals. Meta-reproducibility studies on a set of related papers. Meta analysis on the state of reproducibility in various subfields in Machine Learning. Important Dates Submit to TMLR OpenReview: https://openreview.net/group?id=TMLR Deadline to share your intent to submit a TMLR paper to MLRC: February 21st, 2025 at the following form: https://forms.gle/REgwJQBP8ZXQEaJk7 This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn’t already), you should then update the same form with your paper camera ready details. Cutoff deadline for receiving TMLR decisions: June 20th, 2025 Deadline for announcing accepted papers: June 27th, 2025 Conference day: August 21st, 2025 at Princeton University, NJ, USA Camera Ready Process After you have updated the form with your accepted TMLR paper, it will finally undergo a light AC review to verify MLRC compatibility. AC review will determine the submissions eligible for talks and posters, which will be communicated alongside the acceptance decision. Post acceptance, we will reach out to you for logistics of submitting your videos and posters for the conference. For query regarding MLRC 2025, contact us at mlrc-2025@googlegroups.com.\nKoustuv Sinha\nGeneral Chair, MLRC 2025\n","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"ae13e4b55676c49e92519f3f42b8e2a7","permalink":"https://reproml.org/call_for_papers/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/call_for_papers/","section":"call_for_papers","summary":"The Machine Learning Reproducibility Challenge (MLRC 2025) encourages the community to investigate the reproducibility, replicability and generalisability of published claims in top conferences in the literature.\nSubmissions must be first accepted at TMLR to be considered in the MLRC 2025 Proceedings.","tags":null,"title":"Call for Papers","type":"book"},{"authors":null,"categories":null,"content":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.g. on GitHub, GitLab, BitBucket) and anonymize it according to our double blind guidelines. Document your code appropriately Have a README.md file which describes the exact steps to run your code. You can refer to the ML Code Completeness Checklist to write the README file and make sure your code submission is complete. See this blog post on best practices for reproducibility. Compute Resources Google Colaboratory provides free GPU backed Jupyter Notebooks Instructors can apply for Google Cloud credits for their students. Suggested Readings Online Proceedings of MLRC Arvind Narayanan et al, 2023; Talk: Evaluating LLMs is a Minefield ACL 2022 Tutorial on “Towards Reproducible Machine Learning Research in Natural Language Processing” NAACL 2022 Reproducibility Track ML Reproducibility Checklist ML Code Completeness Checklist ML reproducibility tools and best practices Joelle Pineau’s Keynote talk on Reproducibility at NeurIPS 2018 ","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"1db3ba364adb4d30a0f8fd5677683f69","permalink":"https://reproml.org/challenge_resources/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/challenge_resources/","section":"challenge_resources","summary":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.","tags":null,"title":"Resources","type":"book"},{"authors":null,"categories":null,"content":"2023 🎓 Poster : NeurIPS 2024 Poster Sessions, Dec 10th to 15th, 2024, Vancouver, Canada\nGNNInterpreter: A probabilistic generative model-level explanation for Graph Neural Networks; Ana-Maria Vasilcoiu, Batu Helvacioğlu, Thies Kersten, Thijs Stessen; 🎓 Poster Explaining Temporal Graph Models through an Explorer-Navigator Framework, Miklos Hamar, Matey Krastev, Kristiyan Hristov, David Beglou Reproducibility Study of “Explaining RL Decisions with Trajectories”, Clio Feng, Colin Bot, Bart den Boef, Bart Aaldering Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported, Ethan Harvey, Mikhail Petrov, Michael C. Hughes; :mortar_board Poster Reproducibility study of FairAC, Gijs de Jong,Macha Meijer,Derck W.E. Prinzhorn,Harold Ruiter; 🎓 Poster On the Reproducibility of Post-Hoc Concept Bottleneck Models, Nesta Midavaine, Gregory Hok Tjoan Go, Diego Canez, Ioana Simion, Satchit Chatterji; 🎓 Poster Reproducibility Study of “Learning Perturbations to Explain Time Series Predictions, Jiapeng Fan, Paulius Skaigiris, Luke Cadigan, Sebastian Uriel Arias Explaining RL Decisions with Trajectories’: A Reproducibility Study, Karim Ahmed Abdel Sadek, Matteo Nulli, Joan Velja, Jort Vincenti Classwise-Shapley values for data valuation, Markus Semmler, Miguel de Benito Delgado Reproducibility Study of “ITI-GEN: Inclusive Text-to-Image Generation”, Daniel Gallo Fernández, Răzvan-Andrei Matișan, Alejandro Monroy Muñoz, Janusz Partyka; 🎓 Poster Reproducibility study of “Robust Fair Clustering: A Novel Fairness Attack and Defense Framework, Kacper Bartosik, Eren Kocadag, Vincent Loos, Lucas Ponticelli, 🎓 Poster CUDA: Curriculum of Data Augmentation for Long‐Tailed Recognition, Barath Chandran C; 🎓 Poster Reproducibility Study of “Explaining Temporal Graph Models Through an Explorer-Navigator Framework”, Christina Isaicu, Jesse Wonnink, Andreas Berentzen, Helia Ghasemi; 🎓 Poster Reproducibility Study of “Robust Fair Clustering: A Novel Fairness Attack and Defense Framework”, Iason Skylitsis, Zheng Feng, Idries Nasim, Camille Niessink; 🎓 Poster Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers, Fatemeh Nourilenjan Nokabadi, Jean-Francois Lalonde, Christian Gagné; 🎓 Poster Reproducibility study of “LICO: Explainable Models with Language-Image Consistency”, Luan Fletcher, Robert van der Klis, Martin Sedlacek, Stefan Vasilev, Christos Athanasiadis; 🎓 Poster On the Reproducibility of: “Learning Perturbations to Explain Time Series Predictions”, Wouter Bant, Ádám Divák, Jasper Eppink, Floris Six Dijkstra; 🎓 Poster Reproducibility Study: Equal Improvability: A New Fairness Notion Considering the Long-Term Impact, Berkay Chakar,Amina Izbassar,Mina Janićijević,Jakub Tomaszewski; 🎓 Poster Chain-of-Thought Unfaithfulness as Disguised Accuracy, Oliver Bentham, Nathan Stringham, Ana Marasović Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images, Shivank Garg, Manyana Tiwari Studying How to Efficiently and Effectively Guide Models with Explanations” - A Reproducibility Study, Adrian Sauter, Milan Miletić, Ryan Ott, Rohith Saai Pemmasani Prabakaran; 🎓 Poster Reproducibility Study Of Learning Fair Graph Representations Via Automated Data Augmentations, Thijmen Nijdam, Taiki Papandreou-Lazos, Jurgen de Heus, Juell Sprott; 🎓 Poster 2022 ReScience Volume 9 Issue 2\n2021 ReScience Volume 8 Issue 2\n2020 ReScience Volume 7 Issue 2\nNeurIPS 2019 ReScience Volume 6 Issue 2\nICLR 2018 ReScience Volume 5 Issue 2\n","date":1733788800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733788800,"objectID":"92a50dede0f893dc69287f93aedc7715","permalink":"https://reproml.org/proceedings/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/proceedings/","section":"proceedings","summary":"2023 🎓 Poster : NeurIPS 2024 Poster Sessions, Dec 10th to 15th, 2024, Vancouver, Canada\nGNNInterpreter: A probabilistic generative model-level explanation for Graph Neural Networks; Ana-Maria Vasilcoiu, Batu Helvacioğlu, Thies Kersten, Thijs Stessen; 🎓 Poster Explaining Temporal Graph Models through an Explorer-Navigator Framework, Miklos Hamar, Matey Krastev, Kristiyan Hristov, David Beglou Reproducibility Study of “Explaining RL Decisions with Trajectories”, Clio Feng, Colin Bot, Bart den Boef, Bart Aaldering Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported, Ethan Harvey, Mikhail Petrov, Michael C.","tags":null,"title":"Online Proceedings","type":"book"},{"authors":null,"categories":null,"content":"This page consists of some frequently asked questions about the challenge.\nWhere is MLRC 2024? As we described in our announcement blog post, historically we have had one year backdated, as in MLRC 2023 actually happens in 2024, due to incorporating papers published in 2023. As we move on to be an in-person conference, to closer align with the format of ML conferences and also in favor of broadening our scope, we are therefore dropping the version 2024 and moving directly to MLRC 2025.\nIs the submission double blind? Yes, the paper should be double blind, according to TMLR’s submission policies. You should include the code for review, either by adding anonymized codebase in the Supplementary Materials, or link to an Anonymous Github URL.\nWhat is the format of the paper? Please consult TMLR’s author guidelines.\nI need an extension to submit my paper. Our deadline is a recommended date to submit to TMLR. You can still submit post this deadline. It is upto you to ensure we get your decisions in our system by the cutoff deadline of June 20th, 2025.\nMy paper hasn’t received any reviews on TMLR for some time. What should I do? It is up to you to followup with TMLR Action Editors if your paper hasn’t recieved reviews in time. If your Action Editor is unresponsive, you should reach out to the TMLR Editors-In-Chiefs.\nMy paper has recieved reviews, but it hasn’t recieved any decisions at TMLR. Again, it is your responsibility to follow up with TMLR Action Editor to get your decisions by the cutoff date. If your Action Editor is unresponsive, you should reach out to the TMLR Editors-In-Chiefs.\nCan I get travel grant to attend the conference? We have very limited budget for travel grant, which is not guranteed and can only be provided on a case-by-case basis. We strongly recommend to reaching out to your supervisor/company for help regarding travel.\nCan I get invitation letter for US Visa? Yes, please reach out to us at mlrc-2025@googlegroups.com with a proof of your registration and we will provide it to you.\nThis is super exciting, how can I help? Thanks for your interest in our challenge! You can help out by spreading the news. We are looking for organizers and volunteers, if you would like to nominate someone you know or yourself, please contact us through this form.\nI am from industry, can I participate? Yes you are more than welcome to! Please consider sharing the word about the challenge to your peers in your company too!\n","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"9b683c5fbff8d0d24588e34210d33cb8","permalink":"https://reproml.org/faq/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/faq/","section":"faq","summary":"This page consists of some frequently asked questions about the challenge.\nWhere is MLRC 2024? As we described in our announcement blog post, historically we have had one year backdated, as in MLRC 2023 actually happens in 2024, due to incorporating papers published in 2023.","tags":null,"title":"FAQ","type":"book"},{"authors":null,"categories":null,"content":"MLRC utilizes COLM Code of Conduct, which itself utilizes ICLR Code of Conduct, which is restated here.\nWhy The open exchange of ideas, the freedom of thought and expression, and respectful scientific debate are central to the goals of this conference on language modeling; this requires a community and an environment that recognizes and respects the inherent worth of every person.\nWho All participants—attendees, organizers, reviewers, speakers, sponsors, and volunteers at our conference, workshops, and conference-sponsored social events—are required to agree with this Code of Conduct both during the event and on official communication channels, including social media. Organizers will enforce this code, and we expect cooperation from all participants to help ensure a safe and productive environment for everybody.\nScope The conference commits itself to provide an experience for all participants that is free from harassment, bullying, discrimination, and retaliation. This includes offensive comments related to gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion (or lack thereof), politics, technology choices, or any other personal characteristics. Bullying, intimidation, personal attacks, harassment, sustained disruption of talks or other events, and behavior that interferes with another participant’s full participation will not be tolerated. This includes sexual harassment, stalking, following, harassing photography or recording, inappropriate physical contact, unwelcome sexual attention, public vulgar exchanges, and diminutive characterizations, which are all unwelcome in this community.\nThe expected behavior in line with the scope above extends to any format of the conference, including any virtual forms, and to the use of any online tools related to the conference. These include comments on OpenReview within or outside of reviewing periods, conference-wide chat tools, Q\u0026amp;A tools, live stream interactions, and any other forms of virtual interaction. Trolling, use of inappropriate imagery or videos, offensive language either written or in-person over video or audio, unwarranted direct messages (DMs), and extensions of such behavior to tools outside those used by the conference but related to the conference, its program and attendees, are not allowed. In addition, doxxing or revealing any personal information to target any participant will not be tolerated.\nSponsors are equally subject to this Code of Conduct. In particular, sponsors should not use images, activities, or other materials that are of a sexual, racial, or otherwise offensive nature. Sponsor representatives and staff (including volunteers) should not use sexualized clothing/uniforms/costumes or otherwise create a sexualized environment. This code applies both to official sponsors as well as any organization that uses the conference name as branding as part of its activities at or around the conference.\nOutcomes Participants asked by any member of the community to stop any such behavior are expected to comply immediately. If a participant engages in such behavior, the conference organizers may take any action they deem appropriate, including: a formal or informal warning to the offender, expulsion from the conference (either physical expulsion, or termination of access codes) with no refund, barring from participation in future conferences or their organization, reporting the incident to the offender’s local institution or funding agencies, or reporting the incident to local law enforcement. A response of “just joking” will not be accepted; behavior can be harassing without an intent to offend. If action is taken, an appeals process will be made available.\nReporting If you have concerns related to your inclusion at that conference, or observe someone else’s difficulties, or have any other concerns related to inclusion, please email the MLRC PCs at reproducibility.challenge@gmail.com. For online events and tools, there are options to directly report specific chat/text comments, in addition to the above reporting. Complaints and violations will be handled with discretion. Reports made during the conference will be responded to within 24 hours; those at other times in less than two weeks. We are prepared and eager to help participants contact relevant help services, to escort them to a safe location, or to otherwise assist those experiencing harassment to feel safe for the duration of the conference. We gratefully accept feedback from the community on policy and actions; please contact us.\n","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"caebd5ce02d666af6b8edc4387363f06","permalink":"https://reproml.org/code_of_conduct/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/code_of_conduct/","section":"code_of_conduct","summary":"MLRC utilizes COLM Code of Conduct, which itself utilizes ICLR Code of Conduct, which is restated here.\nWhy The open exchange of ideas, the freedom of thought and expression, and respectful scientific debate are central to the goals of this conference on language modeling; this requires a community and an environment that recognizes and respects the inherent worth of every person.","tags":null,"title":"Code of Conduct","type":"book"},{"authors":null,"categories":null,"content":"MLRC utilizes COLM Code of Ethics, which itself utilizes ICLR Code of Ethics, which is restated here.\nResponsible Stewardship The Machine Learning Reproducibility Challenge (MLRC) is committed to promoting good conduct and a reflexive and responsible approach to research and its applications, especially with respect to research and development related to reproducibility, methods, investigations and tools in machine learning and artificial intelligence. This Code is a vital part of our continuing effort to encourage reflection of the wider impacts of work that is considered by the conference, and to encourage work that consistently supports the responsible stewardship of trustworthy research that advances knowledge, public good and well-being.\nAims This Code of Ethics provides general ethical principles, applicable to both individual researchers and to organizations that carry out, fund, host, or are otherwise involved in research, and associated with COLM. The Code should not be seen as prescriptive but as a set of principles to guide ethical, responsible research.\nPositive Action This code applies to all contributors to MLRC, including reviewers, authors, speakers, organizers of the conference, tutorials, workshops, sponsors, and attendees. MLRC’s contributors are expected to acknowledge this code, and whenever necessary, include a discussion in their contributions that expands on the wider impacts of their work, using this Code as one source of ethical considerations.\nGeneral Ethical Principles Contribute to Society and to Human Well-being Researchers must acknowledge that all people globally are stakeholders in computing, and that we should use our skills for the benefit of society, its members, and our natural environment. Research should minimize negative consequences, including threats to health, safety, personal security, and privacy. In order to do so, it must take into consideration a multiplicity of socio-economic factors and geographies. When the interests of multiple groups conflict, the needs of those less advantaged should be given increased attention and priority. Researchers should consider whether the results of their efforts will respect diversity, will be used in socially responsible ways, will meet social needs, and will be broadly accessible. Uphold High Standards of Scientific Excellence Researchers and organizations should strive for excellence when conducting research and aim to produce and disseminate work of the highest quality. This implies a commitment to open enquiry, intellectual rigor, integrity, and collaboration. Findings must be reported accurately and honestly. Researchers must not make deliberately false or misleading claims, fabricate or falsify data, or misrepresent results. Methods and results should be presented in a way that is transparent and reproducible. Where human subjects are involved in the research process (e.g., in direct experiments, or as annotators), the need for ethical approvals from an appropriate ethical review board should be assessed and reported. All contributions to the research must be acknowledged, and agreements relating to intellectual property, publication and authorship must be complied with. Avoid Harm Here, “harm” means negative consequences. Well-intended actions, including those that accomplish desired outcomes, may lead to harm. When that harm is unintended, those responsible are obliged to undo or mitigate the harm as much as possible. Avoiding harm begins with engaging with application domain experts, engagement with the communities that the research is intended to serve, and a careful consideration of potential impacts on all those affected. When harm is an intentional part of the system, those responsible are obligated to ensure that the harm is ethically justified. Harm to the natural environment, whether in the process of producing research or in its application, should also be considered. In all cases, ensure that all harm is minimized. The consequences of data aggregation and emergent properties of systems should be carefully analyzed, including those that can become integrated into the structure of society. Researchers have an additional obligation to report any signs of system risks that might result in harm. For reporting, see the section at the end on Concerns and Remediation. Be Honest, Trustworthy and Transparent Researchers should be honest about their qualifications, and about any limitations in their competence to complete a task. Researchers should provide full disclosure of all pertinent system capabilities, limitations, and potential problems to the appropriate parties, including any party that may deploy the system. Researchers should be open and transparent about any circumstances that might lead to either real or perceived conflicts of interest or otherwise tend to undermine the independence of their judgment. Researchers must consider their competing interests, including from sources of the funding, and report …","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"90aa9c86a11e6af0fe8c1594087ea1a2","permalink":"https://reproml.org/code_of_ethics/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/code_of_ethics/","section":"code_of_ethics","summary":"MLRC utilizes COLM Code of Ethics, which itself utilizes ICLR Code of Ethics, which is restated here.\nResponsible Stewardship The Machine Learning Reproducibility Challenge (MLRC) is committed to promoting good conduct and a reflexive and responsible approach to research and its applications, especially with respect to research and development related to reproducibility, methods, investigations and tools in machine learning and artificial intelligence.","tags":null,"title":"Code of Ethics","type":"book"},{"authors":null,"categories":null,"content":"Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Université de Montréal, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face Stella Biderman, Booz Allen Hamilton, EleutherAI Arvind Narayanan, Princeton University Jesse Dodge, Allen Institute for AI ","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"d17882b7567ca459947dcfaa25c02da6","permalink":"https://reproml.org/board/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/board/","section":"board","summary":"Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Université de Montréal, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face Stella Biderman, Booz Allen Hamilton, EleutherAI Arvind Narayanan, Princeton University Jesse Dodge, Allen Institute for AI ","tags":null,"title":"Advisory Board","type":"book"},{"authors":null,"categories":null,"content":"We are excited to announce the 8th iteration of the Machine Learning Reproducibility Challenge, MLRC 2025, which will also be the first, in-person conference, hosted at Princeton University, New Jersey, USA on August 21st, 2025!\nThe Machine Learning Reproducibility Challenge (MLRC) is an annual conference for reproducibility research in the Machine Learning community. MLRC has been running as an online conference for the last seven years (v1, v2, v3, v4, v5, v6, v7). This limits the incentives to submit to the conference, as online mode doesn’t offer the authors to showcase their work and network among researchers in the same domain. We have been systematically trying to address this issue by improving the submission and publication process, and partnering with several conferences over the years, either by a workshop, or more recently through a Journal-to-Conference mode with NeurIPS for the last couple of iterations.\nThe success of the MLRC poster sessions at these conferences, and the recent success of COLM, motivated us to “graduate” MLRC into an in-person conference, starting this iteration. MLRC 2025 will be a one-day single track conference, with a mix of invited talks, oral presentations, and poster sessions. We hope the conference will provide the much needed avenue for discussing and disseminating reproducibility research and allow participants and attendees to network over a common goal of improving the science of Machine Learning through reproducible methods. We are excited to partner with Princeton University, specifically the Princeton AI Lab for providing us the venue, and to Meta for providing us the funds to conduct such in-person conference.\nAs for the nomenclature of the conference, historically we have had one year backdated, as in MLRC 2023 actually happens in 2024, due to incorporating papers published in 2023. As we move on to be an in-person conference, to closer align with the format of ML conferences and also in favor of broadening our scope, we are therefore dropping the version 2024 and moving directly to MLRC 2025.\nWe therefore announce the call for papers for MLRC 2025. We invite submissions which conduct novel, unpublished research of reproducibility of machine learning methods and literature, including but not limited to :\nMethods and tools to foster reproducibility research in Machine Learning Generalisability of published claims: novel insights and results beyond what was presented in the original paper, from any paper (or set of papers) published in top ML conferences and journals. Meta-reproducibility studies on a set of related papers. Meta analysis on the state of reproducibility in various subfields in Machine Learning. Submissions must be first accepted at TMLR to be considered in the MLRC 2025 Proceedings. Please read the author guidelines and submission guidelines from TMLR to get the submission format and to understand more about the reviewing process. Existing papers related to the scope (with reproducibility certification) already published at TMLR are also welcome for the consideration of the committee.\nWhile TMLR aims to follow a 2-months timeline to complete the review process of its regular submissions, this timeline is not guaranteed. If you haven’t already, we therefore recommend submitting your original paper to TMLR by February 21st, 2025. We aim to announce the accepted papers by June 27th. We have set a cutoff deadline for accepting TMLR decisions one week prior to the announcement deadline, allowing ample time for you to ensure your paper has received the decision at TMLR, and update our forms accordingly. For logistical purposes, this date will be a hard deadline, and unfortunately we would not be able to accommodate any late decisions from TMLR post this date. Therefore, we encourage you to submit early to TMLR, and contact the TMLR Action Editors well in advance if your paper hasn’t been reviewed or is pending decisions. If you miss the cutoff deadline, we encourage you to still go through the TMLR review cycle, as then your paper once published will be eligible for the next year’s iteration (MLRC 2026). If you already have a relevant published TMLR paper which has not been showcased at MLRC 2023, you can directly submit it now to our system for consideration for MLRC 2025.\nImportant dates Submit to TMLR OpenReview: https://openreview.net/group?id=TMLR Deadline to share your intent to submit a TMLR paper to MLRC: February 21st, 2025 at the following form: https://forms.gle/REgwJQBP8ZXQEaJk7 This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn’t already), you should then update the same form with your paper camera ready details. Cutoff deadline for receiving TMLR decisions: June 20th, 2025 Deadline for announcing accepted papers: June 27th, 2025 Conference day: August 21st, 2025 at Princeton University, NJ, USA In the following months, we will share more updates about the conference session, invited …","date":1733958e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733958e3,"objectID":"23f7d02125b97dd39772e0681d450ca4","permalink":"https://reproml.org/blog/announcing_mlrc2025/","publishdate":"2024-12-12T00:00:00+01:00","relpermalink":"/blog/announcing_mlrc2025/","section":"blog","summary":"We are excited to announce the 8th iteration of the Machine Learning Reproducibility Challenge, MLRC 2025, which will also be the first, in-person conference, hosted at Princeton University, New Jersey, USA on August 21st, 2025!","tags":null,"title":"Announcing MLRC 2025, our first in-person conference","type":"book"},{"authors":null,"categories":null,"content":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences. As we gear up to announce the seventh iteration of the challenge, MLRC 2023, we would like to share our learnings from previous years and how we plan to incorporate these lessons into the upcoming challenge.\nRetrospectives Over the years, the ML Reproducibility Challenge has been largely aimed at students beginning their journey in ML research as part of their ML coursework. This provided an accessible entry point to ML research, allowing early career researchers to participate and learn a full paper publication lifecycle - from designing the research question to investigating the limits of a scientific hypothesis to final publication acceptance. One of the success stories of this approach was from the University of Amsterdam, which designed a course around the challenge (FACT AI), and consistently produced high quality reproducibility reports in the final proceedings of the challenge. While the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nWhile the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nReproducibility is not a binary outcome The term “reproducibility” unfortunately comes with a baggage - whenever we talk about a paper to be reproducible, the expectation is this binary property - yes or no. However, the reality is way more nuanced: a paper presents multiple hypotheses (claims) of varying importance to the central claim - which some of them can be directly reproducible, others might not be; some of the claims may even be limited in terms “generalisability”. Consequently, we consistently found the quality of the reports submitted to the challenge fall into either of these two categories: a) making a sweeping claim about reproducibility, or b) diving deep and constructing a holistic view of reproducibility, replicability and generalisability of the claims presented in the original paper. Not surprisingly, the latter cohort is always highly rated by the reviewers and ends up more often in the accepted pool. From the 2020 iteration, we introduced a Reproducibility Summary template to encourage participants to focus on the central claims of the paper, and to mainly focus on this generalisability aspect - results beyond the original paper. We found that introducing this template helps the authors to focus more on these questions, thereby improving their submission.\nReproducibility is not about whether author’s code gives the same results Thanks to the continued effort made by the ML community in terms of Checklists and mandatory code submission policies, we now see \u0026gt;90% of papers accompanied by their source code. This is a very promising progress regarding reproducibility of the research in our field - the presence of code alleviates many questions and issues regarding the implementation, thereby facilitating exact reproducibility. Inadvertently, this also resulted in many MLRC submissions where authors only run the provided code and compare the numbers. While these contributions measure replicability, they are not strong research contributions which add valuable insights to the field. Instead, strong submissions tend to leverage the authors code to make exhaustive ablations, hyperparameter search and explore generalisability results on different data/models.\nRedundant reproductions of the same resource-friendly papers For several years, we find that authors tend to pick papers which are more resource-friendly - i.e. papers which can run on a single commodity GPU. This is likely a side-effect of the challenge being targeted primarily towards early career researchers. While reproducibility study on such resource-light papers is not a problem per se, it does often result in multiple reproduction reports on the same paper. We hypothesize that this is probably due to courses assigning multiple groups to work on a single paper, in order to better manage logistics. As we did not have any deduplication criteria, we explicitly inform our reviewers to not penalize multiple …","date":1697583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697583600,"objectID":"04c6a40295a0c6d20b98b1c300f0d6ae","permalink":"https://reproml.org/blog/announcing_mlrc2023/","publishdate":"2023-10-18T00:00:00+01:00","relpermalink":"/blog/announcing_mlrc2023/","section":"blog","summary":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences.","tags":null,"title":"Announcing MLRC 2023","type":"book"},{"authors":null,"categories":null,"content":"Final decisions for MLRC 2023 We are now releasing the final list of decisions for MLRC 2023. This list includes the previous partial list published on July 5th, 2024. We have given additional time to TMLR to complete the reviews, however it is unfortunate that few papers are still awaiting a decision due to unresponsive Action Editors from TMLR. As we need to wrap up this edition, we are proceeding with the final list of 22 accepted papers. Congratulations to all!\nIf you are an author of the below mentioned papers and have not submitted the form with the camera ready items, please consider doing so at the earliest. We will reach out to the accepted authors soon with the next steps. We will also announce the best paper awards and share details on the logistics of NeurIPS poster session in the coming weeks.\nUpdate, Sept 13th, 2024: A couple of papers received acceptance status post our final date of MLRC 2023 acceptance. We have now incorporated them too in the final list.\nAn update on decisions July 5th, 2024\nWe initially communicated to have all decisions of MLRC 2023 out by 31st of May, 2024. Unfortunately, several submissions are still under review at TMLR, and we are waiting for the final decisions to trickle in. Overall, MLRC 2023 had 46 valid submissions, out of which we have recieved decisions on 61% of them. We are in touch with TMLR to expedite the process of decisions for the remaining submissions - we expect all decisions to come in by the next couple of weeks.\nUntil then, we are happy to announce the (partial) list of accepted papers. Congratulations to all 🎉! If you are an author of the below mentioned papers and have not submitted the form with the camera ready items, please consider doing so at the earliest. We will reach out to the accepted authors soon with the next steps.\n(partial paper list removed as we release the final list above)\n[Deprecated] Call For Papers We invite contributions from academics, practitioners and industry researchers of the ML community to submit novel and insightful reproducibility studies. Please read our blog post regarding our retrospectives of running the challenge and the future roadmap. We are happy to announce the formal partnership with Transactions of Machine Learning Research (TMLR) journal. The challenge goes live on October 23, 2023.\nWe recommend you choose any paper(s) published in the 2023 calendar year from the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ICCV, CVPR, TMLR, JMLR, TACL) to run your reproducibility study on.\nIn order for your paper to be submitted and presented at MLRC 2023, it first needs to be accepted and published at TMLR. While TMLR aims to follow a 2-months timeline to complete the review process of its regular submissions, this timeline is not guaranteed. If you haven’t already, we therefore recommend submitting your original paper to TMLR by February 16th, 2024, that is a little over 3 months in advance of the MLRC publication announcement date.\nKey Dates Challenge goes live: October 23, 2023 Deadline to share your intent to submit a TMLR paper to MLRC: February 16th, 2024 at the following form: https://forms.gle/JJ28rLwBSxMriyE89. This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn’t already), you should then update the same form with your paper camera ready details. Your accepted TMLR paper will finally undergo a light AC review to verify MLRC compatibility. We aim to announce the accepted papers by May 31st, 2024 July 17th, 2024, pending decisions of all papers. Contact Information For query regarding MLRC 2023, mail us at: mlrc-2023@googlegroups.com. For general queries, media, sponsorship, partnership requests, mail us at reproducibility.challenge@gmail.com. Organizing Committee Koustuv Sinha, Meta (FAIR) Jessica Zosa Forde, Brown University Mandana Samiei, McGill University, Mila Arna Ghosh, McGill University, Mila Lintang Sutawika, Eleuther AI Siba Smarak Panigrahi, McGill University, Mila ","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"3c5c4824ac010698afd303a3b61b9807","permalink":"https://reproml.org/proceedings/mlrc2023/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/proceedings/mlrc2023/","section":"proceedings","summary":"Final decisions for MLRC 2023 We are now releasing the final list of decisions for MLRC 2023. This list includes the previous partial list published on July 5th, 2024. We have given additional time to TMLR to complete the reviews, however it is unfortunate that few papers are still awaiting a decision due to unresponsive Action Editors from TMLR.","tags":null,"title":"MLRC 2023","type":"book"}]