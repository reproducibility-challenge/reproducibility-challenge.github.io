[{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b0d8ccffb8d530bff08365a148a04ee6","permalink":"https://example.com/blog/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/","section":"blog","summary":"","tags":null,"title":"Blog","type":"book"},{"authors":null,"categories":null,"content":"Registration for RC2023 Registration and project submissions will be handled by our Link\nList of Partner venues You can choose to reproduce papers published in the calendar year of 2022 for the following venues:\nPreparing Your Report You have to use the Reproducibility Summary template for writing your reports. The submission is strictly double blind, any violation of this will lead to desk reject. The first page of your report must incorporate this summary template, which should consist of the following sections:\nScope of reproducibility: Briefly define the claim of the paper that you intend to tackle in this report Methodology: Briefly state the overall methodology used in your approach Results: Concisely state the results / findings of the reproducibility study What was easy: State which part of the reproducibility study was easy to do. What was difficult: State the difficulties that you encountered while performing the reproducibility study. You can find the Reproducibility Summary template here. The first page of the template is mandatory and you can organize the rest of the report as you deem fit. We have also released a recommended outline of the report which you can use to incorporate your findings. You can find the Reproducibility Summary template here. The first page of the template is mandatory and you can organize the rest of the report as you deem fit. We have also released a recommended outline of the report which you can use to incorporate your findings.\nSubmitting your report We will use the same OpenReview Portal to submit your Reproducibility projects. Click on “Add Reproducibility Challenge Report” to submit your work. In the field of “Paper URL”, paste the paper URL of the paper (from the appropriate conference proceedings). In the Abstract, copy paste the Reproducibility Summary from your report.\nPlease check Important Dates for the deadline for submission of your report. Your report should be of maximum 9 pages excluding references, in ReScience style format as specified in Reproducibility Summary template. You can also submit supplementary materials. Code submission is mandatory along with your supplementary materials.\nReviewing Criteria We will be selecting reports which present a clear, well motivated, and thorough reproducibility effort. The report should very briefly discuss the main ideas of the paper and not simply reiterate the main work. Your report should clearly mention the key findings of your effort, and if necessary include discussion / commentary / suggestions to the readers re-implementing this work on its replicability / complexity. Your report must contain the Reproducibility Summary in the first page as defined in the previous section.\nReports will be reviewed on the following criteria. You can also refer to the reviewing criteria used in MLRC 2021 for reference.\nReproducibility Summary Concise Problem Statement Reproducible code Communication with authors Hyperparameter Search Ablation Study Discussion on results Recommendations for reproducibility Results beyond the paper Clarity of writing ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"75baf1ce134166b964c054ce6737061a","permalink":"https://example.com/registration/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/registration/","section":"registration","summary":"Registration for RC2023 Registration and project submissions will be handled by our Link\nList of Partner venues You can choose to reproduce papers published in the calendar year of 2022 for the following venues:","tags":null,"title":"Registration","type":"book"},{"authors":null,"categories":null,"content":"Task Description You can select a paper from the list of accepted papers from the list of partner conferences and aim to replicate the main claim described in the paper. The objective is to assess if the experiments are reproducible, and to determine if the conclusions of the paper are supported by your findings. Your results can be either positive (i.e. confirm reproducibility), or negative (i.e. explain what you were unable to reproduce, and potentially explain why).\nEssentially, think of your role as an inspector verifying the validity of the experimental results and conclusions of the paper. In some instances, your role will also extend to helping the authors improve the quality of their work and paper.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"9879af85fac65dbb1d14d31e494e59ac","permalink":"https://example.com/task/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/task/","section":"task","summary":"Task Description You can select a paper from the list of accepted papers from the list of partner conferences and aim to replicate the main claim described in the paper. The objective is to assess if the experiments are reproducible, and to determine if the conclusions of the paper are supported by your findings.","tags":null,"title":"Task","type":"book"},{"authors":null,"categories":null,"content":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted reports. We recommend that you:\nPublish your code in a repository (e.g. on GitHub, GitLab, BitBucket) and anonymize it according to our double blind guidelines. Document your code appropriately Have a README.md file which describes the exact steps to run your code. You can refer to the ML Code Completeness Checklist to write the README file and make sure your code submission is complete. See this blog post on best practices for reproducibility. Compute Resources Google Colaboratory provides free GPU backed Jupyter Notebooks Code Ocean provides 10hrs/month of GPU accelerated platform free to academics. Code Ocean is a cloud-based research collaboration platform. Instructors can apply for Google Cloud credits for their students. Each student will be given a fixed amount of starter credits to use Google Cloud Compute (GCP). Students can also request credits from Google Cloud Compute. If you are a company that can offer cloud computing credits, please contact reproducibility.challenge@gmail.com Suggested Readings Past accepted papers of MLRC at the ReScience Journal (Volume 8 Issue 2, Volume 7 Issue 2, Volume 6 Issue 2, Volume 5 Issue 2) [For course instructors] ACL 2022 Tutorial on “Towards Reproducible Machine Learning Research in Natural Language Processing” NAACL 2022 Reproducibility Track ML Reproducibility Checklist ML Code Completeness Checklist ML reproducibility tools and best practices Joelle Pineau’s Keynote talk on Reproducibility at NeurIPS 2018 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"1db3ba364adb4d30a0f8fd5677683f69","permalink":"https://example.com/challenge_resources/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/challenge_resources/","section":"challenge_resources","summary":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted reports. We recommend that you:\nPublish your code in a repository (e.","tags":null,"title":"Resources","type":"book"},{"authors":null,"categories":null,"content":"This page consists of some frequently asked questions about the challenge.\nCan I reproduce a workshop paper from the listed conferences? Unfortunately no, as of now we are not accepting reports on workshop papers from the listed conferences. Please work on any accepted papers from the conference proceedings.\nI want to reproduce a paper from a conference not listed in the challenge. Can I? Before starting to work on it, please contact the organizers to confirm, as your request will be evaluated on a case-by-case basis.\nI want to reproduce a paper from a previous year conference paper. Can I? Typically we do allow it if the paper is published within a couple of years prior to the current scope of conferences. Although, before starting to work on it, please contact the organizers to confirm, as your request will be evaluated on a case-by-case basis.\nHow many team members are allowed? Any number of team members are allowed.\nDo all team members have to be from the same institution? Not at all, we encourage cross-institute participation! In that case, add the institutions of your team members in a comma separated format in the claims and report submission forms in OpenReview.\nDo I have to add team members based on author order? No, author order will depend on your final report.\nDo I need to be registered at a course to participate? No, you can participate independently on your own. Participation from industry is especially welcome!\nI am a course instructor, how do I participate officially with my course? Many thanks for your participation! You can just drop us a mail (reproducibility.challenge@gmail.com) with details of your course, and we will list it on our website!\nI am a course instructor, can I forward my reviews on the reports of my class? Yes! If your course ends earlier than the submission deadline and you have already graded the assignments, you can directly send us the evaluations! Have your students submit their report in our OpenReview portal, and drop us a mail at reproducibility.challenge@gmail.com with the evaluations paired with the submission links.\nI am a student participant from a course, but I do not see my course listed? Please contact your course instructor or TA to send us a mail (reproducibility.challenge@gmail.com) to register your course. We will update the website periodically and add new courses.\nI am from industry, can I participate? Yes you are more than welcome to! Please consider sharing the word about the challenge to your peers in your company too!\nThis is all nice, but what do I gain from participating? You will first add to the knowledge of the original paper. Peer reviewed reports will be showcased on PapersWithCode and published in the ReScience Journal. Starting this year, we are partnering with NeurIPS conference to showcase the accepted papers as a poster session in the main conference. Please check the following announcement for details.\nWhere can I get GPUs to run experiments? Check the Resources tab for more information.\nCan I contact the authors? Yes! It is highly recommended to contact the authors of the paper you are reproducing, to clarify doubts and implementation details.\nHow do I contact the authors? You can send the authors mail directly to initiate a discussion. The contact details can be found on the paper, which is linked in the pdf of the paper which is available for each paper.\nHow do I get the code of the paper? You can either search the pdf of the paper for the code, or find the link to PapersWithCode page of the paper, which is usually updated with the publicly released code of the authors.\nHow much of the code am I allowed to use? There is no restriction on the extent of the original code you can use for the reproducibility effort.\nIs the submission double blind? Yes, the report to be submitted should be double blind. When submitting code for review, include your codebase in the Supplementary Materials, or link to an Anonymous Github URL.\nWhat is the format of the report? You can find the style files of the report in this template.\nIs the Reproducibility Summary section of the report mandatory? Yes! You should use our style files and add the “Reproducibility Summary” in the first page of your report. Make sure this summary does not exceed the first page. Failure of adding this summary will result in desk rejection.\nDoes the Reproducibility Summary count towards the page limit? Yes, the Reproducibility Summary counts towards the total page limit (nine pages).\nWhen I am submitting the report, what should I write in the “Abstract” field? You should copy your Reproducibility Summary in the abstract field. You should add a line separator between sections, and ensure the summary is properly formatted.\nIf my report is selected, how do I submit to ReScience? Once your report is accepted, you will be required to submit the final draft by camera-ready deadline. Details of this process will be communicated to you after acceptance notification.\nThis is super …","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"9b683c5fbff8d0d24588e34210d33cb8","permalink":"https://example.com/faq/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/faq/","section":"faq","summary":"This page consists of some frequently asked questions about the challenge.\nCan I reproduce a workshop paper from the listed conferences? Unfortunately no, as of now we are not accepting reports on workshop papers from the listed conferences.","tags":null,"title":"FAQ","type":"book"},{"authors":null,"categories":null,"content":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences. As we gear up to announce the seventh iteration of the challenge, MLRC 2023, we would like to share our learnings from previous years and how we plan to incorporate these lessons into the upcoming challenge.\nRetrospectives Over the years, the ML Reproducibility Challenge has been largely aimed at students beginning their journey in ML research as part of their ML coursework. This provided an accessible entry point to ML research, allowing early career researchers to participate and learn a full paper publication lifecycle - from designing the research question to investigating the limits of a scientific hypothesis to final publication acceptance. One of the success stories of this approach was from the University of Amsterdam, which designed a course around the challenge (FACT AI), and consistently produced high quality reproducibility reports in the final proceedings of the challenge. While the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nWhile the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nReproducibility is not a binary outcome The term “reproducibility” unfortunately comes with a baggage - whenever we talk about a paper to be reproducible, the expectation is this binary property - yes or no. However, the reality is way more nuanced: a paper presents multiple hypotheses (claims) of varying importance to the central claim - which some of them can be directly reproducible, others might not be; some of the claims may even be limited in terms “generalisability”. Consequently, we consistently found the quality of the reports submitted to the challenge fall into either of these two categories: a) making a sweeping claim about reproducibility, or b) diving deep and constructing a holistic view of reproducibility, replicability and generalisability of the claims presented in the original paper. Not surprisingly, the latter cohort is always highly rated by the reviewers and ends up more often in the accepted pool. From the 2020 iteration, we introduced a Reproducibility Summary template to encourage participants to focus on the central claims of the paper, and to mainly focus on this generalisability aspect - results beyond the original paper. We found that introducing this template helps the authors to focus more on these questions, thereby improving their submission.\nReproducibility is not about whether author’s code gives the same results Thanks to the continued effort made by the ML community in terms of Checklists and mandatory code submission policies, we now see \u0026gt;90% of papers accompanied by their source code. This is a very promising progress regarding reproducibility of the research in our field - the presence of code alleviates many questions and issues regarding the implementation, thereby facilitating exact reproducibility. Inadvertently, this also resulted in many MLRC submissions where authors only run the provided code and compare the numbers. While these contributions measure replicability, they are not strong research contributions which add valuable insights to the field. Instead, strong submissions tend to leverage the authors code to make exhaustive ablations, hyperparameter search and explore generalisability results on different data/models.\nRedundant reproductions of the same resource-friendly papers For several years, we find that authors tend to pick papers which are more resource-friendly - i.e. papers which can run on a single commodity GPU. This is likely a side-effect of the challenge being targeted primarily towards early career researchers. While reproducibility study on such resource-light papers is not a problem per se, it does often result in multiple reproduction reports on the same paper. We hypothesize that this is probably due to courses assigning multiple groups to work on a single paper, in order to better manage logistics. As we did not have any deduplication criteria, we explicitly inform our reviewers to not penalize multiple …","date":1697583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697583600,"objectID":"04c6a40295a0c6d20b98b1c300f0d6ae","permalink":"https://example.com/blog/announcing_mlrc2023/","publishdate":"2023-10-18T00:00:00+01:00","relpermalink":"/blog/announcing_mlrc2023/","section":"blog","summary":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences.","tags":null,"title":"Announcing MLRC 2023","type":"book"}]