[{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b0d8ccffb8d530bff08365a148a04ee6","permalink":"https://reproml.org/blog/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/","section":"blog","summary":"","tags":null,"title":"Blog","type":"book"},{"authors":null,"categories":null,"content":" The Machine Learning Reproducibility Challenge (MLRC 2025) encourages the community to investigate the reproducibility, replicability and generalisability of published claims in top conferences in the literature.\nSubmissions must be first accepted at TMLR to be considered in the MLRC 2025 Proceedings. Please read the author guidelines and submission guidelines from TMLR to get the submission format and to understand more about the reviewing process. Existing papers related to the scope (with reproducibility certification) already published at TMLR are also welcome for the consideration of the committee. Please read our announcement blog post for more information.\nScope We invite submissions which conduct novel, unpublished research of reproducibility of machine learning methods and literature, including but not limited to :\nMethods and tools to foster reproducibility research in Machine Learning Generalisability of published claims: novel insights and results beyond what was presented in the original paper, from any paper (or set of papers) published in top ML conferences and journals. Meta-reproducibility studies on a set of related papers. Meta analysis on the state of reproducibility in various subfields in Machine Learning. Important Dates Submit to TMLR OpenReview: https://openreview.net/group?id=TMLR Deadline to share your intent to submit a TMLR paper to MLRC: February 21st, 2025 This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn‚Äôt already), you should then update the same form with your paper camera ready details. Cutoff deadline for receiving TMLR decisions: June 20th, 2025 Deadline for announcing accepted papers: June 27th, 2025 Conference day: August 21st, 2025 at Princeton University, NJ, USA Camera Ready Process After you have updated the form with your accepted TMLR paper, it will finally undergo a light AC review to verify MLRC compatibility. AC review will determine the submissions eligible for talks and posters, which will be communicated alongside the acceptance decision. Post acceptance, we will reach out to you for logistics of submitting your videos and posters for the conference. For query regarding MLRC 2025, contact us at mlrc-2025@googlegroups.com.\nKoustuv Sinha\nGeneral Chair, MLRC 2025\n","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"ae13e4b55676c49e92519f3f42b8e2a7","permalink":"https://reproml.org/call_for_papers/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/call_for_papers/","section":"call_for_papers","summary":"The Machine Learning Reproducibility Challenge (MLRC 2025) encourages the community to investigate the reproducibility, replicability and generalisability of published claims in top conferences in the literature.\nSubmissions must be first accepted at TMLR to be considered in the MLRC 2025 Proceedings.","tags":null,"title":"Call for Papers","type":"book"},{"authors":null,"categories":null,"content":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.g. on GitHub, GitLab, BitBucket) and anonymize it according to our double blind guidelines. Document your code appropriately Have a README.md file which describes the exact steps to run your code. You can refer to the ML Code Completeness Checklist to write the README file and make sure your code submission is complete. See this blog post on best practices for reproducibility. Compute Resources Google Colaboratory provides free GPU backed Jupyter Notebooks Instructors can apply for Google Cloud credits for their students. Suggested Readings Online Proceedings of MLRC Arvind Narayanan et al, 2023; Talk: Evaluating LLMs is a Minefield ACL 2022 Tutorial on ‚ÄúTowards Reproducible Machine Learning Research in Natural Language Processing‚Äù NAACL 2022 Reproducibility Track ML Reproducibility Checklist ML Code Completeness Checklist ML reproducibility tools and best practices Joelle Pineau‚Äôs Keynote talk on Reproducibility at NeurIPS 2018 ","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"1db3ba364adb4d30a0f8fd5677683f69","permalink":"https://reproml.org/challenge_resources/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/challenge_resources/","section":"challenge_resources","summary":"This page lists some useful resources which you can use for the challenge.\nReproducible Code Code submission are mandatory for all submitted papers. We recommend that you:\nPublish your code in a repository (e.","tags":null,"title":"Resources","type":"book"},{"authors":null,"categories":null,"content":"##[2025]\nDo not trust what you trust: Miscalibration in Semisupervised Learning, Shambhavi Mishra, Balamurali Murugesan Ismail Ben Ayed, Marco Pedersoli, Jose Dolz Multivariate Dense Retrieval: A Reproducibility Study under a Memory-limited Setup Georgios Sidiropoulos, Samarth Bhargav, Panagiotis Eustratiadis, Evangelos Kanoulas Improving Interpretation Faithfulness for Vision Transformers Izabela Kurek, Wojciech Trejter, Stipe Frkovic, Andro Erdelez Reassessing Fairness: A Reproducibility Study of NIFA‚Äôs Impact on GNN Models Ruben Figge, Sjoerd Gunneweg, Aaron Kuin, Mees Lindeman A reproducibility study of ‚ÄúUser-item fairness tradeoffs in recommendations‚Äù Sander Honig, Elyanne Oey, Lisanne Wallaard, Sharanda Suttorp, Clara Rus Reproducibility study of: ‚ÄúCompetition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals‚Äù Tijs Wiegman, Leyla Perotti, Vikt√≥ria Pravdov√°, Ori Brand, Maria Heuss On the Generalizability of ‚ÄúCompetition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals‚Äù Asen Dotsinski, Udit Thakur, Marko Ivanov, Mohammad Hafeez Khan, Maria Heuss Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models Atharv Mittal, Agam Pandey, Amritanshu Tiwari, Sukrit Jindal, Swadesh Swain Reproducibility Study of ‚ÄúCooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation‚Äù Jose L. Garc√≠a, Karol√≠na H√°jkov√°, Maria Marchenko, Carlos Miguel Pati√±o Remembering to Be Fair Again: Reproducing Non-Markovian Fairness in Sequential Decision Making Domonkos Nagy, Lohithsai Yadala Chanchu, Krystof Bobek, Xin Zhou, Jacobus Smit Reproducibility Study of ‚ÄôSLICE: Stabilized LIME for Consistent Explanations for Image Classification‚Äô Aritra Bandyopadhyay, Chiranjeev Bindra, Roan van Blanken, Arijit Ghosh GNNBoundary: Finding Boundaries and Going Beyond Them Jan Henrik Bertrand, Lukas Bierling, Ina Klaric, Aron Wezenberg Revisiting Discover-then-Name Concept Bottleneck Models: A Reproducibility Study Freek Byrman, Emma Kasteleyn, Bart Kuipers, Daniel Uyterlinde Benchmarking LLM Capabilities in Negotiation through Scoreable Games Jorge Carrasco Pollo, Ioannis Kapetangeorgis, Joshua Rosenthal, John Hua Yao ModernTCN Revisited: A Critical Look at the Experimental Setup in General Time Series Analysis √ñnder Akacik,Mark Hoogendoorn Reproducibility Study of ‚ÄúImproving Interpretation Faithfulness For Vision Transformers‚Äù Meher Changlani, Benjamin Hucko, Aswin Krishna Mahadevan, Ioannis Kechagias A Reproducibility Study of Decoupling Feature Extraction and Classification Layers for Calibrated Neural Networks Eric Banzuzi, Johanna D‚Äôciofalo Khodaverdian, Katharina Deckenbach 2023 üéì Poster : NeurIPS 2024 Poster Sessions, Dec 10th to 15th, 2024, Vancouver, Canada\nGNNInterpreter: A probabilistic generative model-level explanation for Graph Neural Networks; Ana-Maria Vasilcoiu, Batu Helvacioƒülu, Thies Kersten, Thijs Stessen; üéì Poster Explaining Temporal Graph Models through an Explorer-Navigator Framework, Miklos Hamar, Matey Krastev, Kristiyan Hristov, David Beglou Reproducibility Study of ‚ÄúExplaining RL Decisions with Trajectories‚Äù, Clio Feng, Colin Bot, Bart den Boef, Bart Aaldering Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported, Ethan Harvey, Mikhail Petrov, Michael C. Hughes; :mortar_board Poster Reproducibility study of FairAC, Gijs de Jong,Macha Meijer,Derck W.E. Prinzhorn,Harold Ruiter; üéì Poster On the Reproducibility of Post-Hoc Concept Bottleneck Models, Nesta Midavaine, Gregory Hok Tjoan Go, Diego Canez, Ioana Simion, Satchit Chatterji; üéì Poster Reproducibility Study of ‚ÄúLearning Perturbations to Explain Time Series Predictions, Jiapeng Fan, Paulius Skaigiris, Luke Cadigan, Sebastian Uriel Arias Explaining RL Decisions with Trajectories‚Äô: A Reproducibility Study, Karim Ahmed Abdel Sadek, Matteo Nulli, Joan Velja, Jort Vincenti Classwise-Shapley values for data valuation, Markus Semmler, Miguel de Benito Delgado Reproducibility Study of ‚ÄúITI-GEN: Inclusive Text-to-Image Generation‚Äù, Daniel Gallo Fern√°ndez, RƒÉzvan-Andrei Mati»ôan, Alejandro Monroy Mu√±oz, Janusz Partyka; üéì Poster Reproducibility study of ‚ÄúRobust Fair Clustering: A Novel Fairness Attack and Defense Framework, Kacper Bartosik, Eren Kocadag, Vincent Loos, Lucas Ponticelli, üéì Poster CUDA: Curriculum of Data Augmentation for Long‚ÄêTailed Recognition, Barath Chandran C; üéì Poster Reproducibility Study of ‚ÄúExplaining Temporal Graph Models Through an Explorer-Navigator Framework‚Äù, Christina Isaicu, Jesse Wonnink, Andreas Berentzen, Helia Ghasemi; üéì Poster Reproducibility Study of ‚ÄúRobust Fair Clustering: A Novel Fairness Attack and Defense Framework‚Äù, Iason Skylitsis, Zheng Feng, Idries Nasim, Camille Niessink; üéì Poster Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers, Fatemeh Nourilenjan Nokabadi, Jean-Francois Lalonde, Christian Gagn√©; üéì Poster ‚Ä¶","date":1733788800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733788800,"objectID":"92a50dede0f893dc69287f93aedc7715","permalink":"https://reproml.org/proceedings/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/proceedings/","section":"proceedings","summary":"##[2025]\nDo not trust what you trust: Miscalibration in Semisupervised Learning, Shambhavi Mishra, Balamurali Murugesan Ismail Ben Ayed, Marco Pedersoli, Jose Dolz Multivariate Dense Retrieval: A Reproducibility Study under a Memory-limited Setup Georgios Sidiropoulos, Samarth Bhargav, Panagiotis Eustratiadis, Evangelos Kanoulas Improving Interpretation Faithfulness for Vision Transformers Izabela Kurek, Wojciech Trejter, Stipe Frkovic, Andro Erdelez Reassessing Fairness: A Reproducibility Study of NIFA‚Äôs Impact on GNN Models Ruben Figge, Sjoerd Gunneweg, Aaron Kuin, Mees Lindeman A reproducibility study of ‚ÄúUser-item fairness tradeoffs in recommendations‚Äù Sander Honig, Elyanne Oey, Lisanne Wallaard, Sharanda Suttorp, Clara Rus Reproducibility study of: ‚ÄúCompetition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals‚Äù Tijs Wiegman, Leyla Perotti, Vikt√≥ria Pravdov√°, Ori Brand, Maria Heuss On the Generalizability of ‚ÄúCompetition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals‚Äù Asen Dotsinski, Udit Thakur, Marko Ivanov, Mohammad Hafeez Khan, Maria Heuss Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models Atharv Mittal, Agam Pandey, Amritanshu Tiwari, Sukrit Jindal, Swadesh Swain Reproducibility Study of ‚ÄúCooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation‚Äù Jose L.","tags":null,"title":"Online Proceedings","type":"book"},{"authors":null,"categories":null,"content":"This page consists of some frequently asked questions about the challenge.\nWhere is MLRC 2024? As we described in our announcement blog post, historically we have had one year backdated, as in MLRC 2023 actually happens in 2024, due to incorporating papers published in 2023. As we move on to be an in-person conference, to closer align with the format of ML conferences and also in favor of broadening our scope, we are therefore dropping the version 2024 and moving directly to MLRC 2025.\nIs the submission double blind? Yes, the paper should be double blind, according to TMLR‚Äôs submission policies. You should include the code for review, either by adding anonymized codebase in the Supplementary Materials, or link to an Anonymous Github URL.\nWhat is the format of the paper? Please consult TMLR‚Äôs author guidelines.\nI need an extension to submit my paper. Our deadline is a recommended date to submit to TMLR. You can still submit post this deadline. It is upto you to ensure we get your decisions in our system by the cutoff deadline of June 20th, 2025.\nMy paper hasn‚Äôt received any reviews on TMLR for some time. What should I do? It is up to you to followup with TMLR Action Editors if your paper hasn‚Äôt recieved reviews in time. If your Action Editor is unresponsive, you should reach out to the TMLR Editors-In-Chiefs.\nMy paper has recieved reviews, but it hasn‚Äôt recieved any decisions at TMLR. Again, it is your responsibility to follow up with TMLR Action Editor to get your decisions by the cutoff date. If your Action Editor is unresponsive, you should reach out to the TMLR Editors-In-Chiefs.\nCan I get invitation letter for US Visa? Yes, please reach out to us at mlrc-2025@googlegroups.com with a proof of your registration and we will provide it to you.\nThis is super exciting, how can I help? Thanks for your interest in our challenge! You can help out by spreading the news. We are looking for organizers and volunteers, if you would like to nominate someone you know or yourself, please contact us through this form.\nI am from industry, can I participate? Yes you are more than welcome to! Please consider sharing the word about the challenge to your peers in your company too!\n","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"9b683c5fbff8d0d24588e34210d33cb8","permalink":"https://reproml.org/faq/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/faq/","section":"faq","summary":"This page consists of some frequently asked questions about the challenge.\nWhere is MLRC 2024? As we described in our announcement blog post, historically we have had one year backdated, as in MLRC 2023 actually happens in 2024, due to incorporating papers published in 2023.","tags":null,"title":"FAQ","type":"book"},{"authors":null,"categories":null,"content":"MLRC follows Princeton Code of Conduct\n","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"caebd5ce02d666af6b8edc4387363f06","permalink":"https://reproml.org/code_of_conduct/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/code_of_conduct/","section":"code_of_conduct","summary":"MLRC follows Princeton Code of Conduct","tags":null,"title":"Code of Conduct","type":"book"},{"authors":null,"categories":null,"content":"MLRC follows Princeton Code of Conduct\n","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"90aa9c86a11e6af0fe8c1594087ea1a2","permalink":"https://reproml.org/code_of_ethics/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/code_of_ethics/","section":"code_of_ethics","summary":"MLRC follows Princeton Code of Conduct","tags":null,"title":"Code of Ethics","type":"book"},{"authors":null,"categories":null,"content":"Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Universit√© de Montr√©al, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face Stella Biderman, Booz Allen Hamilton, EleutherAI Arvind Narayanan, Princeton University Jesse Dodge, Allen Institute for AI ","date":1733961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1733961600,"objectID":"d17882b7567ca459947dcfaa25c02da6","permalink":"https://reproml.org/board/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/board/","section":"board","summary":"Reproducible ML Advisory Board Joelle Pineau, Meta (FAIR), McGill University, Mila Hugo Larochelle, Google Deepmind, Universit√© de Montr√©al, Mila Robert Stojnic, Meta Sasha Luccioni, Hugging Face Stella Biderman, Booz Allen Hamilton, EleutherAI Arvind Narayanan, Princeton University Jesse Dodge, Allen Institute for AI ","tags":null,"title":"Advisory Board","type":"book"},{"authors":null,"categories":null,"content":"We are excited to announce the 8th iteration of the Machine Learning Reproducibility Challenge, MLRC 2025, organized by Princeton Laboratory for Artificial Intelligence, which will also be the first, in-person conference, hosted at Princeton University, New Jersey, USA on August 21st, 2025!\nThe Machine Learning Reproducibility Challenge (MLRC) is an annual conference for reproducibility research in the Machine Learning community. MLRC has been running as an online conference for the last seven years (v1, v2, v3, v4, v5, v6, v7). This limits the incentives to submit to the conference, as online mode doesn‚Äôt offer the authors to showcase their work and network among researchers in the same domain. We have been systematically trying to address this issue by improving the submission and publication process, and partnering with several conferences over the years, either by a workshop, or more recently through a Journal-to-Conference mode with NeurIPS for the last couple of iterations.\nThe success of the MLRC poster sessions at these conferences, and the recent success of COLM, motivated us to ‚Äúgraduate‚Äù MLRC into an in-person conference, starting this iteration. MLRC 2025 will be a one-day single track conference, with a mix of invited talks, oral presentations, and poster sessions. We hope the conference will provide the much needed avenue for discussing and disseminating reproducibility research and allow participants and attendees to network over a common goal of improving the science of Machine Learning through reproducible methods. We are excited to partner with Princeton University, and thank Princeton AI Lab \u0026amp; Arvind Narayanan for facilitating the venue.\nAs for the nomenclature of the conference, historically we have had one year backdated, as in MLRC 2023 actually happens in 2024, due to incorporating papers published in 2023. As we move on to be an in-person conference, to closer align with the format of ML conferences and also in favor of broadening our scope, we are therefore dropping the version 2024 and moving directly to MLRC 2025.\nWe therefore announce the call for papers for MLRC 2025. We invite submissions which conduct novel, unpublished research of reproducibility of machine learning methods and literature, including but not limited to :\nMethods and tools to foster reproducibility research in Machine Learning Generalisability of published claims: novel insights and results beyond what was presented in the original paper, from any paper (or set of papers) published in top ML conferences and journals. Meta-reproducibility studies on a set of related papers. Meta analysis on the state of reproducibility in various subfields in Machine Learning. Submissions must be first accepted at TMLR to be considered in the MLRC 2025 Proceedings. Please read the author guidelines and submission guidelines from TMLR to get the submission format and to understand more about the reviewing process. Existing papers related to the scope (with reproducibility certification) already published at TMLR are also welcome for the consideration of the committee.\nWhile TMLR aims to follow a 2-months timeline to complete the review process of its regular submissions, this timeline is not guaranteed. If you haven‚Äôt already, we therefore recommend submitting your original paper to TMLR by February 21st, 2025. We aim to announce the accepted papers by June 27th. We have set a cutoff deadline for accepting TMLR decisions one week prior to the announcement deadline, allowing ample time for you to ensure your paper has received the decision at TMLR, and update our forms accordingly. For logistical purposes, this date will be a hard deadline, and unfortunately we would not be able to accommodate any late decisions from TMLR post this date. Therefore, we encourage you to submit early to TMLR, and contact the TMLR Action Editors well in advance if your paper hasn‚Äôt been reviewed or is pending decisions. If you miss the cutoff deadline, we encourage you to still go through the TMLR review cycle, as then your paper once published will be eligible for the next year‚Äôs iteration (MLRC 2026). If you already have a relevant published TMLR paper which has not been showcased at MLRC 2023, you can directly submit it now to our system for consideration for MLRC 2025.\nImportant dates Submit to TMLR OpenReview: https://openreview.net/group?id=TMLR Deadline to share your intent to submit a TMLR paper to MLRC: February 21st, 2025 at the following form: https://forms.gle/REgwJQBP8ZXQEaJk7 This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn‚Äôt already), you should then update the same form with your paper camera ready details. Cutoff deadline for receiving TMLR decisions: June 20th, 2025 Deadline for announcing accepted papers: June 27th, 2025 Conference day: August 21st, 2025 at Princeton University, NJ, USA In the following months, we will share more updates about the conference session, invited talks, ‚Ä¶","date":1733958e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733958e3,"objectID":"23f7d02125b97dd39772e0681d450ca4","permalink":"https://reproml.org/blog/announcing_mlrc2025/","publishdate":"2024-12-12T00:00:00+01:00","relpermalink":"/blog/announcing_mlrc2025/","section":"blog","summary":"We are excited to announce the 8th iteration of the Machine Learning Reproducibility Challenge, MLRC 2025, organized by Princeton Laboratory for Artificial Intelligence, which will also be the first, in-person conference, hosted at Princeton University, New Jersey, USA on August 21st, 2025!","tags":null,"title":"Announcing MLRC 2025, our first in-person conference","type":"book"},{"authors":null,"categories":null,"content":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences. As we gear up to announce the seventh iteration of the challenge, MLRC 2023, we would like to share our learnings from previous years and how we plan to incorporate these lessons into the upcoming challenge.\nRetrospectives Over the years, the ML Reproducibility Challenge has been largely aimed at students beginning their journey in ML research as part of their ML coursework. This provided an accessible entry point to ML research, allowing early career researchers to participate and learn a full paper publication lifecycle - from designing the research question to investigating the limits of a scientific hypothesis to final publication acceptance. One of the success stories of this approach was from the University of Amsterdam, which designed a course around the challenge (FACT AI), and consistently produced high quality reproducibility reports in the final proceedings of the challenge. While the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nWhile the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.\nReproducibility is not a binary outcome The term ‚Äúreproducibility‚Äù unfortunately comes with a baggage - whenever we talk about a paper to be reproducible, the expectation is this binary property - yes or no. However, the reality is way more nuanced: a paper presents multiple hypotheses (claims) of varying importance to the central claim - which some of them can be directly reproducible, others might not be; some of the claims may even be limited in terms ‚Äúgeneralisability‚Äù. Consequently, we consistently found the quality of the reports submitted to the challenge fall into either of these two categories: a) making a sweeping claim about reproducibility, or b) diving deep and constructing a holistic view of reproducibility, replicability and generalisability of the claims presented in the original paper. Not surprisingly, the latter cohort is always highly rated by the reviewers and ends up more often in the accepted pool. From the 2020 iteration, we introduced a Reproducibility Summary template to encourage participants to focus on the central claims of the paper, and to mainly focus on this generalisability aspect - results beyond the original paper. We found that introducing this template helps the authors to focus more on these questions, thereby improving their submission.\nReproducibility is not about whether author‚Äôs code gives the same results Thanks to the continued effort made by the ML community in terms of Checklists and mandatory code submission policies, we now see \u0026gt;90% of papers accompanied by their source code. This is a very promising progress regarding reproducibility of the research in our field - the presence of code alleviates many questions and issues regarding the implementation, thereby facilitating exact reproducibility. Inadvertently, this also resulted in many MLRC submissions where authors only run the provided code and compare the numbers. While these contributions measure replicability, they are not strong research contributions which add valuable insights to the field. Instead, strong submissions tend to leverage the authors code to make exhaustive ablations, hyperparameter search and explore generalisability results on different data/models.\nRedundant reproductions of the same resource-friendly papers For several years, we find that authors tend to pick papers which are more resource-friendly - i.e. papers which can run on a single commodity GPU. This is likely a side-effect of the challenge being targeted primarily towards early career researchers. While reproducibility study on such resource-light papers is not a problem per se, it does often result in multiple reproduction reports on the same paper. We hypothesize that this is probably due to courses assigning multiple groups to work on a single paper, in order to better manage logistics. As we did not have any deduplication criteria, we explicitly inform our reviewers to not penalize multiple ‚Ä¶","date":1697583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697583600,"objectID":"04c6a40295a0c6d20b98b1c300f0d6ae","permalink":"https://reproml.org/blog/announcing_mlrc2023/","publishdate":"2023-10-18T00:00:00+01:00","relpermalink":"/blog/announcing_mlrc2023/","section":"blog","summary":"Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences.","tags":null,"title":"Announcing MLRC 2023","type":"book"},{"authors":null,"categories":null,"content":"Final decisions for MLRC 2023 We are now releasing the final list of decisions for MLRC 2023. This list includes the previous partial list published on July 5th, 2024. We have given additional time to TMLR to complete the reviews, however it is unfortunate that few papers are still awaiting a decision due to unresponsive Action Editors from TMLR. As we need to wrap up this edition, we are proceeding with the final list of 22 accepted papers. Congratulations to all!\nIf you are an author of the below mentioned papers and have not submitted the form with the camera ready items, please consider doing so at the earliest. We will reach out to the accepted authors soon with the next steps. We will also announce the best paper awards and share details on the logistics of NeurIPS poster session in the coming weeks.\nUpdate, Sept 13th, 2024: A couple of papers received acceptance status post our final date of MLRC 2023 acceptance. We have now incorporated them too in the final list.\nAn update on decisions July 5th, 2024\nWe initially communicated to have all decisions of MLRC 2023 out by 31st of May, 2024. Unfortunately, several submissions are still under review at TMLR, and we are waiting for the final decisions to trickle in. Overall, MLRC 2023 had 46 valid submissions, out of which we have recieved decisions on 61% of them. We are in touch with TMLR to expedite the process of decisions for the remaining submissions - we expect all decisions to come in by the next couple of weeks.\nUntil then, we are happy to announce the (partial) list of accepted papers. Congratulations to all üéâ! If you are an author of the below mentioned papers and have not submitted the form with the camera ready items, please consider doing so at the earliest. We will reach out to the accepted authors soon with the next steps.\n(partial paper list removed as we release the final list above)\n[Deprecated] Call For Papers We invite contributions from academics, practitioners and industry researchers of the ML community to submit novel and insightful reproducibility studies. Please read our blog post regarding our retrospectives of running the challenge and the future roadmap. We are happy to announce the formal partnership with Transactions of Machine Learning Research (TMLR) journal. The challenge goes live on October 23, 2023.\nWe recommend you choose any paper(s) published in the 2023 calendar year from the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ICCV, CVPR, TMLR, JMLR, TACL) to run your reproducibility study on.\nIn order for your paper to be submitted and presented at MLRC 2023, it first needs to be accepted and published at TMLR. While TMLR aims to follow a 2-months timeline to complete the review process of its regular submissions, this timeline is not guaranteed. If you haven‚Äôt already, we therefore recommend submitting your original paper to TMLR by February 16th, 2024, that is a little over 3 months in advance of the MLRC publication announcement date.\nKey Dates Challenge goes live: October 23, 2023 Deadline to share your intent to submit a TMLR paper to MLRC: February 16th, 2024 at the following form: https://forms.gle/JJ28rLwBSxMriyE89. This form requires that you provide a link to your TMLR submission. Once it gets accepted (if it isn‚Äôt already), you should then update the same form with your paper camera ready details. Your accepted TMLR paper will finally undergo a light AC review to verify MLRC compatibility. We aim to announce the accepted papers by May 31st, 2024 July 17th, 2024, pending decisions of all papers. Contact Information For query regarding MLRC 2023, mail us at: mlrc-2023@googlegroups.com. For general queries, media, sponsorship, partnership requests, mail us at reproducibility.challenge@gmail.com. Organizing Committee Koustuv Sinha, Meta (FAIR) Jessica Zosa Forde, Brown University Mandana Samiei, McGill University, Mila Arna Ghosh, McGill University, Mila Lintang Sutawika, Eleuther AI Siba Smarak Panigrahi, McGill University, Mila ","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"3c5c4824ac010698afd303a3b61b9807","permalink":"https://reproml.org/proceedings/mlrc2023/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/proceedings/mlrc2023/","section":"proceedings","summary":"Final decisions for MLRC 2023 We are now releasing the final list of decisions for MLRC 2023. This list includes the previous partial list published on July 5th, 2024. We have given additional time to TMLR to complete the reviews, however it is unfortunate that few papers are still awaiting a decision due to unresponsive Action Editors from TMLR.","tags":null,"title":"MLRC 2023","type":"book"}]