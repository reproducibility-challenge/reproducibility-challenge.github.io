<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=description content="Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences."><link rel=alternate hreflang=en-us href=https://example.com/blog/announcing_mlrc2023/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.8e71bb65194d79d1ceeeb05a4f651870.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://example.com/blog/announcing_mlrc2023/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@repro_ml"><meta property="twitter:creator" content="@repro_ml"><meta property="og:site_name" content="MLRC2023"><meta property="og:url" content="https://example.com/blog/announcing_mlrc2023/"><meta property="og:title" content="Announcing MLRC 2023 | MLRC2023"><meta property="og:description" content="Every year since 2018, we have conducted the Machine Learning Reproducibility Challenge (see previous years v1, v2, v3, v4, v5, v6), which invites the ML community to examine the reproducibility of existing, recently published papers in top conferences."><meta property="og:image" content="https://example.com/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://example.com/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-10-18T00:00:00+01:00"><meta property="article:modified_time" content="2023-10-18T00:00:00+01:00"><title>Announcing MLRC 2023 | MLRC2023</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=04c6a40295a0c6d20b98b1c300f0d6ae><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>MLRC2023</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>MLRC2023</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span>Home</span></a></li><li class=nav-item><a class=nav-link href=https://forms.gle/JJ28rLwBSxMriyE89 target=_blank rel=noopener><span>Submission Form</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Past Iterations</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://paperswithcode.com/rc2022><span>MLRC 2022</span></a>
<a class=dropdown-item href=https://paperswithcode.com/rc2021><span>MLRC 2021</span></a>
<a class=dropdown-item href=https://paperswithcode.com/rc2020><span>MLRC 2020</span></a>
<a class=dropdown-item href=https://reproml.org/neurips2019/><span>NeurIPS 2019</span></a>
<a class=dropdown-item href=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html><span>ICLR 2019</span></a>
<a class=dropdown-item href=https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html><span>ICLR 2018</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Blog</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>Search...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/>ML Reproducibility Challenge 2023</a></li></ul><div class=docs-toc-item><a class=docs-toc-link href=/blog/><i class="fas fa-task-square-svgrepo-com pr-1"></i>Blog</a><ul class="nav docs-sidenav"></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/call_for_papers/><i class="fas fa-task-square-svgrepo-com pr-1"></i>Call for Papers</a></div><div class=docs-toc-item><a class=docs-toc-link href=/challenge_resources/><i class="fas fa-books-svgrepo-com pr-1"></i>Resources</a></div><div class=docs-toc-item><a class=docs-toc-link href=/faq/><i class="fas fa-faq-file-svgrepo-com pr-1"></i>FAQ</a></div><div class=docs-toc-item><a class=docs-toc-link href=/proceedings/><i class="fas fa-faq-file-svgrepo-com pr-1"></i>Online Proceedings</a></div><div class=docs-toc-item><a class=docs-toc-link href=/organizers/><i class="fas fa-task-square-svgrepo-com pr-1"></i>Organizers</a></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#retrospectives>Retrospectives</a><ul><li><a href=#reproducibility-is-not-a-binary-outcome>Reproducibility is not a binary outcome</a></li><li><a href=#reproducibility-is-not-about-whether-authors-code-gives-the-same-results>Reproducibility is not about whether author’s code gives the same results</a></li><li><a href=#redundant-reproductions-of-the-same-resource-friendly-papers>Redundant reproductions of the same resource-friendly papers</a></li><li><a href=#low-signal-reviews-due-to-inexperienced-reviewers>Low signal reviews due to inexperienced reviewers</a></li><li><a href=#low-incentives-to-publish-a-reproducibility-report>Low incentives to publish a reproducibility report</a></li></ul></li><li><a href=#on-the-road-ahead>On the road ahead</a><ul><li><a href=#broadening-the-target-audience>Broadening the target audience</a></li><li><a href=#increasing-the-bar-of-submissions>Increasing the bar of submissions</a></li><li><a href=#implementing-a-comprehensive-and-open-reviewing-cycle>Implementing a comprehensive and open reviewing cycle</a></li><li><a href=#improving-incentives-to-participate-in-the-challenge>Improving incentives to participate in the challenge</a></li><li><a href=#providing-a-new-home-for-mlrc-web>Providing a new home for MLRC web</a></li></ul></li><li><a href=#mlrc-2023-call-for-papers>MLRC 2023 Call for Papers</a></li><li><a href=#closing-thoughts>Closing Thoughts</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><article class=article><div class=docs-article-container><nav class="d-none d-md-flex" aria-label=breadcrumb><ol class=breadcrumb><li class=breadcrumb-item><a href=/>Home</a></li><li class=breadcrumb-item><a href=/blog/>Blog</a></li><li class="breadcrumb-item active" aria-current=page>Announcing MLRC 2023</li></ol></nav></div><div class=docs-article-container><h1>Announcing MLRC 2023</h1><div class=article-style><p>Every year since 2018, we have conducted the Machine Learning Reproducibility
Challenge (see previous years
<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html target=_blank rel=noopener>v1</a>,
<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html target=_blank rel=noopener>v2</a>,
<a href=https://reproducibility-challenge.github.io/neurips2019/ target=_blank rel=noopener>v3</a>,
<a href=https://reproducibility-challenge.github.io/neurips2019/ target=_blank rel=noopener>v4</a>,
<a href=https://paperswithcode.com/rc2021 target=_blank rel=noopener>v5</a>,
<a href=https://paperswithcode.com/rc2022 target=_blank rel=noopener>v6</a>), which invites the ML community to
examine the reproducibility of existing, recently published papers in top
conferences. As we gear up to announce the seventh iteration of the challenge,
MLRC 2023, we would like to share our learnings from previous years and how we
plan to incorporate these lessons into the upcoming challenge.</p><h2 id=retrospectives>Retrospectives</h2><p>Over the years, the ML Reproducibility Challenge has been largely aimed at
students beginning their journey in ML research as part of their ML coursework.
This provided an accessible entry point to ML research, allowing early career
researchers to participate and learn a full paper publication lifecycle - from
designing the research question to investigating the limits of a scientific
hypothesis to final publication acceptance. One of the success stories of this
approach was from the University of Amsterdam, which designed a course around
the challenge
(<a href=https://studiegids.uva.nl/xmlpages/page/2022-2023-en/search-course/course/99121 target=_blank rel=noopener>FACT AI</a>),
and consistently produced high quality reproducibility reports in the final
proceedings of the challenge. While the challenge is a valuable resource to ML
course instructors and early career ML researchers, we are eager for the
challenge to grow in scope and impact. More specifically, we want to encourage
ML researchers to contribute novel research that will improve scientific
practice and understanding in the field. We thus identified several shortcomings
of the current model following our retrospection of the submitted and accepted
papers in the challenge.</p><p>While the challenge is a valuable resource to ML course instructors and early
career ML researchers, we are eager for the challenge to grow in scope and
impact. More specifically, we want to encourage ML researchers to contribute
novel research that will improve scientific practice and understanding in the
field. We thus identified several shortcomings of the current model following
our retrospection of the submitted and accepted papers in the challenge.</p><h3 id=reproducibility-is-not-a-binary-outcome>Reproducibility is not a binary outcome</h3><p>The term “reproducibility” unfortunately comes with a baggage - whenever we talk
about a paper to be reproducible, the expectation is this binary property - yes
or no. However, the reality is way more nuanced: a paper presents multiple
hypotheses (claims) of varying importance to the central claim - which some of
them can be directly reproducible, others might not be; some of the claims may
even be limited in terms
<a href=https://dl.acm.org/doi/abs/10.5555/3546258.3546422 target=_blank rel=noopener>“generalisability”</a>.
Consequently, we consistently found the quality of the reports submitted to the
challenge fall into either of these two categories: a) making a sweeping claim
about reproducibility, or b) diving deep and constructing a holistic view of
reproducibility, replicability and generalisability of the claims presented in
the original paper. Not surprisingly, the latter cohort is always highly rated
by the reviewers and ends up more often in the accepted pool. From the
<a href=https://paperswithcode.com/rc2020/registration target=_blank rel=noopener>2020 iteration</a>, we introduced
a Reproducibility Summary template to encourage participants to focus on the
central claims of the paper, and to mainly focus on this generalisability
aspect - results beyond the original paper. We found that introducing this
template helps the authors to focus more on these questions, thereby improving
their submission.</p><h3 id=reproducibility-is-not-about-whether-authors-code-gives-the-same-results>Reproducibility is not about whether author’s code gives the same results</h3><p>Thanks to the continued effort made by the ML community in terms of Checklists
and mandatory code submission policies, we now see >90% of papers accompanied by
their source code. This is a very promising progress regarding reproducibility
of the research in our field - the presence of code alleviates many questions
and issues regarding the implementation, thereby facilitating exact
reproducibility. Inadvertently, this also resulted in many MLRC submissions
where authors only run the provided code and compare the numbers. While these
contributions measure replicability, they are not strong research contributions
which add valuable insights to the field. Instead, strong submissions tend to
leverage the authors code to make exhaustive ablations, hyperparameter search
and explore generalisability results on different data/models.</p><h3 id=redundant-reproductions-of-the-same-resource-friendly-papers>Redundant reproductions of the same resource-friendly papers</h3><p>For several years, we find that authors tend to pick papers which are more
resource-friendly - i.e. papers which can run on a single commodity GPU. This is
likely a side-effect of the challenge being targeted primarily towards early
career researchers. While reproducibility study on such resource-light papers is
not a problem per se, it does often result in multiple reproduction reports on
the same paper. We hypothesize that this is probably due to courses assigning
multiple groups to work on a single paper, in order to better manage logistics.
As we did not have any deduplication criteria, we explicitly inform our
reviewers to not penalize multiple reproducibility reports on the same paper. We
aimed to reduce this by introducing a pre-registration phase early on
(<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html target=_blank rel=noopener>2019</a>,
<a href=https://paperswithcode.com/rc2020/registration target=_blank rel=noopener>2020</a>), however that turned out
to be logistically challenging leading us to discontinue it. In our opinion,
cherry-picking the same paper reduces the breadth of papers being reproduced in
the challenge, invites duplication in work and overall lessens the scientific
contribution to the community.</p><h3 id=low-signal-reviews-due-to-inexperienced-reviewers>Low signal reviews due to inexperienced reviewers</h3><p>Reviewing for the ML Reproducibility Challenge is unique - it requires the
reviewer to first read and understand the original paper(s) and then perform a
critical judgment of the reproducibility report. Hence, workload wise, reviewing
for this challenge requires twice the amount of time per paper than a standard
ML conference. We therefore typically try to evenly reduce the reviewing
workload, with a maximum of two papers per reviewer. Over the last several
iterations, barring from the top reviewers, we observed a concerning trend of
low signal reviews. We hypothesize this mainly due to the different format and
higher workload. To remedy this, we have introduced comprehensive reviewer
guidelines, and also awarded top reviewer awards to further incentivize high
quality reviews. We are grateful to our reviewers for their consistent support,
and we have observed a steady number of reviewers who consistently provide high
quality, useful reviews and hence feature in the top reviewers list on multiple
occasions.</p><h3 id=low-incentives-to-publish-a-reproducibility-report>Low incentives to publish a reproducibility report</h3><p>From the inception of the challenge, we have partnered with ReScience as our
journal publication medium. <a href=https://rescience.github.io/ target=_blank rel=noopener>ReScience</a> is a peer
reviewed, open journal focusing on reproducibility reports across many different
fields of computational science, making it a unique venue. ReScience journal
editorial process is open and live on <a href=https://github.com/ReScience target=_blank rel=noopener>Github</a>,
making it very convenient to access. However, we have observed that the
popularity of ReScience in the Machine Learning community is still low, limiting
the incentives of publication at the challenge. Furthermore, we found ReScience
journal entries are not yet
<a href=https://github.com/ReScience/rescience.github.io/issues/113 target=_blank rel=noopener>properly indexed</a>
by Google Scholar, although the editors are working hard to fix that. Another
issue was since MLRC is not a workshop at any major conference, the original
format did not have any option to present papers to the community, hurting the
incentives even further. Since 2022, we have partnered with
<a href=https://blog.neurips.cc/2022/08/15/journal-showcase/ target=_blank rel=noopener>NeurIPS</a> to allow poster
presentations of accepted papers at the Journal to Conference Track, which
significantly increases the incentive and prestige of publishing papers at MLRC.
We have also partnered with
<a href=https://www.kaggle.com/reproducibility-challenge-2022 target=_blank rel=noopener>Kaggle</a> in our last
iteration to provide accepted papers compute credits to further incentivize
submission and high quality research. Authors of top papers were granted a
significant amount of compute credits by Kaggle to further pursue their
research.</p><h2 id=on-the-road-ahead>On the road ahead</h2><p>We want to continue improving the challenge on the following aspects: broadening
the target audience, broadening the scope and improving incentives, to make the
challenge more exciting to the community and encourage reproducible research.</p><p>We are thus happy to announce the formal partnership with
<a href=https://jmlr.org/tmlr/ target=_blank rel=noopener>Transactions of Machine Learning Research (TMLR)</a>
journal. TMLR is a new journal in the ML community, which is under the umbrella
of Journal of Machine Learning Research (JMLR), and has been fast growing in
significance and reputation within the field. Unlike JMLR, TMLR caters to
shorter format manuscripts similar to conference proceedings, and employs a fast
and open reviewing cycle, ensuring high quality submissions. Therefore, in the
upcoming iteration (MLRC 2023), papers will be published at TMLR instead of
ReScience.</p><h3 id=broadening-the-target-audience>Broadening the target audience</h3><p>While the MLRC will still be useful for the early-career researchers in ML
courses, we want to expand and encourage submissions from the broader community,
including academia and industry. Since TMLR publication accounts for
significantly high prestige and reception in the ML community, we hope this
change would attract a broad range of researchers to contribute to the
advancement of our understanding of reproducibility.</p><h3 id=increasing-the-bar-of-submissions>Increasing the bar of submissions</h3><p>As we look forward, the focus of a reproducibility paper should be much more
than mere reproduction - it should ideally investigate the generalisability of
the original claims. Results and investigations beyond what the authors proposed
are therefore encouraged, which adds to the novelty of the contribution. We
discourage simple reproduction work - while they are useful, they do not provide
enough value to the community. Submissions having multi-paper, topic-based
focused contributions are preferred over single paper reproductions. Novel work
on tools to investigate and enable reproducible research are also welcome to the
submission. We also recommend you to read TMLR’s
<a href=https://jmlr.org/tmlr/editorial-policies.html target=_blank rel=noopener>submission guidelines and
editorial policies</a> which also
applies equally to MLRC submissions.</p><h3 id=implementing-a-comprehensive-and-open-reviewing-cycle>Implementing a comprehensive and open reviewing cycle</h3><p>As we partner with TMLR, we also leverage their open, comprehensive reviewing
mechanism. Papers submitted to MLRC would first undergo TMLR’s reviewing
process. TMLR employs rich and diverse reviewers from the ML community, along
with expert Action Editors. Reviews will be viewed publicly on
<a href="https://openreview.net/group?id=TMLR" target=_blank rel=noopener>OpenReview</a>, and TMLR comes with a quick
reviewing turnaround which includes author rebuttals - a highly requested
feature in our previous iterations.</p><h3 id=improving-incentives-to-participate-in-the-challenge>Improving incentives to participate in the challenge</h3><p>Publication of MLRC papers at TMLR will improve the reception and dissemination
of the work in the broader ML community. Accepted papers at TMLR are announced
in mailing lists and social media on
<a href=https://jmlr.org/tmlr/contact.html target=_blank rel=noopener>a regular basis</a>. Papers accepted at TMLR
are indexed in Google Scholar using the existing OpenReview mechanism, allowing
easy citations and tracking cited counts. We also hope to continue our existing
partnership with NeurIPS to present accepted papers in the Journal to Conference
Showcase Track, allowing further dissemination and opportunity to gain feedback
from the ML community. (If you are attending NeurIPS 2023 in person, checkout
the Journal to Conference Track poster session for MLRC 2022 accepted papers!)</p><h3 id=providing-a-new-home-for-mlrc-web>Providing a new home for MLRC web</h3><p>We are happy to announce our new and permanent online home,
<a href=http://reproml.org target=_blank rel=noopener>reproml.org</a>. Announcements, information and blog posts
about MLRC 2023 and all subsequent iterations will be hosted in this dedicated
space. We are grateful to PapersWithCode for providing online hosting for our
past three iterations!</p><h2 id=mlrc-2023-call-for-papers>MLRC 2023 Call for Papers</h2><p>Finally, we are happy to formally announce MLRC 2023, which will go live
starting on <strong>October 23rd</strong>! We invite contributions from academics,
practitioners and industry researchers of the ML community to submit novel and
insightful reproducibility studies. Please read our
<a href=/call_for_papers/>Call for Papers</a> for more information.</p><p>We recommend you choose any paper(s) published in the 2023 calendar year from
the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ECCV, CVPR,
TMLR, JMLR, TACL) to run your reproducibility study on.</p><figure class=mlrc_dark><div class="d-flex justify-content-center"><div class=w-100><img src=../../uploads/mlrc.drawio.svg alt loading=lazy data-zoomable></div></div></figure><figure class=mlrc_light><div class="d-flex justify-content-center"><div class=w-100><img src=../../uploads/mlrc.light.drawio.svg alt loading=lazy data-zoomable></div></div></figure><p>In order for your paper to be submitted and presented at MLRC 2023, it first
needs to be <strong>accepted and published</strong> at TMLR. While TMLR aims to follow a
2-months timeline to complete the review process of its regular submissions,
this timeline is not guaranteed. If you haven’t already, we therefore recommend
submitting your original paper to TMLR by <strong>February 16th, 2024</strong>, that is a
little over 3 months in advance of the MLRC publication announcement date.</p><ul><li>Challenge goes live: October 23, 2023</li><li>Deadline to share your intent to submit a TMLR paper to MLRC: February 16th,
2024 <strong>Form: <a href=https://forms.gle/JJ28rLwBSxMriyE89 target=_blank rel=noopener>https://forms.gle/JJ28rLwBSxMriyE89</a></strong>. This form requires that
you provide a link to your TMLR submission. Once it gets accepted (if it isn’t
already), you should then update the same form with your paper camera ready
details.</li><li>Your accepted TMLR paper will finally undergo a light AC review to verify MLRC
compatibility.</li><li>We aim to announce the accepted papers by May 31st, 2024, pending decisions of
all papers.</li></ul><h2 id=closing-thoughts>Closing Thoughts</h2><p>As we begin a new era of reproducibility research in Machine Learning, we hope
our continued quest for high quality reproducibility studies will inspire the
community to not only investigate the claims of existing papers, but add novel
research insights and contributions to the literature, accelerating the progress
of science. We hope these steps towards improving the incentives of investing in
reproducibility research enables the community to produce higher quality
scientific contributions.</p></div><div class=article-widget><div class=post-nav></div></div></div><div class=body-footer><p>Last updated on Oct 18, 2023</p><p class=edit-page><a href=https://github.com/wowchemy/hugo-documentation-theme/edit/main/content/blog/announcing_mlrc2023/index.md><i class="fas fa-pen pr-2"></i>Edit this page</a></p></div></article><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 ML Reproducibility Challenge.</p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.85290d887e7fcdd400ccb3ffb9bcd3e3.js></script></body></html>