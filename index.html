<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap" media=print onload='this.media="all"'><meta name=description content="Machine Learning Reproducibility Challenge"><link rel=alternate hreflang=en-us href=https://example.com/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.42054d4e6d5962eeb18e50b9aee03b7e.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title=MLRC2023><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu01e218cfd9d302fe44ebd3672696cf1d_8740_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu01e218cfd9d302fe44ebd3672696cf1d_8740_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://example.com/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@repro_ml"><meta property="twitter:creator" content="@repro_ml"><meta property="og:site_name" content="MLRC2023"><meta property="og:url" content="https://example.com/"><meta property="og:title" content="ML Reproducibility Challenge 2023 | MLRC2023"><meta property="og:description" content="Machine Learning Reproducibility Challenge"><meta property="og:image" content="https://example.com/media/icon_hu01e218cfd9d302fe44ebd3672696cf1d_8740_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://example.com/media/icon_hu01e218cfd9d302fe44ebd3672696cf1d_8740_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-10-22T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://example.com/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://example.com/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Project","@id":"https://example.com/","name":"MLRC2023","logo":"https://example.com/media/icon_hu01e218cfd9d302fe44ebd3672696cf1d_8740_192x192_fill_lanczos_center_3.png","url":"https://example.com/"}</script><title>ML Reproducibility Challenge 2023 | MLRC2023</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper data-wc-page-id=3976528693a0108357f4928017600865><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>MLRC2023</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>MLRC2023</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class="nav-link active" href=/><span>Home</span></a></li><li class=nav-item><a class=nav-link href=https://forms.gle/JJ28rLwBSxMriyE89 target=_blank rel=noopener><span>Submission Form</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Past Iterations</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://paperswithcode.com/rc2022 data-target=https://paperswithcode.com/rc2022><span>MLRC 2022</span></a>
<a class=dropdown-item href=https://paperswithcode.com/rc2021 data-target=https://paperswithcode.com/rc2021><span>MLRC 2021</span></a>
<a class=dropdown-item href=https://paperswithcode.com/rc2020 data-target=https://paperswithcode.com/rc2020><span>MLRC 2020</span></a>
<a class=dropdown-item href=https://reproml.org/neurips2019/ data-target=https://reproml.org/neurips2019/><span>NeurIPS 2019</span></a>
<a class=dropdown-item href=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html data-target=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html><span>ICLR 2019</span></a>
<a class=dropdown-item href=https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html data-target=https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html><span>ICLR 2018</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Home</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>Search...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li class=active><a href=/>ML Reproducibility Challenge 2023</a></li></ul><div class=docs-toc-item><a class=docs-toc-link href=/blog/><i class="fas fa-task-square-svgrepo-com pr-1"></i>Blog</a><ul class="nav docs-sidenav"></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/call_for_papers/><i class="fas fa-task-square-svgrepo-com pr-1"></i>Call for Papers</a></div><div class=docs-toc-item><a class=docs-toc-link href=/challenge_resources/><i class="fas fa-books-svgrepo-com pr-1"></i>Resources</a></div><div class=docs-toc-item><a class=docs-toc-link href=/faq/><i class="fas fa-faq-file-svgrepo-com pr-1"></i>FAQ</a></div><div class=docs-toc-item><a class=docs-toc-link href=/proceedings/><i class="fas fa-faq-file-svgrepo-com pr-1"></i>Online Proceedings</a></div><div class=docs-toc-item><a class=docs-toc-link href=/organizers/><i class="fas fa-task-square-svgrepo-com pr-1"></i>Organizers</a></div></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><article class=article><div class=docs-article-container></div><div class=docs-article-container><h1>ML Reproducibility Challenge 2023</h1><div class=article-style><p>Welcome to the ML Reproducibility Challenge 2023 (<strong>MLRC 2023</strong>). This is the
seventh edition of the event
(<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html target=_blank rel=noopener>v1</a>,
<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html target=_blank rel=noopener>v2</a>,
<a href=https://reproducibility-challenge.github.io/neurips2019/ target=_blank rel=noopener>v3</a>,
<a href=https://paperswithcode.com/rc2020 target=_blank rel=noopener>v4</a>,
<a href=https://paperswithcode.com/rc2021 target=_blank rel=noopener>v5</a>,
<a href=https://paperswithcode.com/rc2022 target=_blank rel=noopener>v6</a>). The primary goal of this event is to
encourage the publishing and sharing of scientific results that are reliable and
reproducible. In support of this, the objective of this challenge is to
investigate reproducibility of papers accepted for publication at top
conferences by inviting members of the community at large to select a paper, and
verify the empirical results and claims in the paper by reproducing the
computational experiments, either via a new implementation or using code/data or
other information provided by the authors.</p><h2 id=final-decisions-for-mlrc-2023>Final decisions for MLRC 2023</h2><p>We are now releasing the final list of decisions for MLRC 2023. This list
includes the previous partial list published on July 5th, 2024. We have given
additional time to TMLR to complete the reviews, however it is unfortunate that
few papers are still awaiting a decision due to unresponsive Action Editors from
TMLR. As we need to wrap up this edition, we are proceeding with the final list
of 22 accepted papers. Congratulations to all!</p><ul><li>Ana-Maria Vasilcoiu, Batu Helvacioğlu, Thies Kersten, Thijs Stessen;
<em>GNNInterpreter: A probabilistic generative model-level explanation for Graph
Neural Networks</em>, <a href="https://openreview.net/forum?id=8cYcR23WUo" target=_blank rel=noopener>OpenReview</a></li><li>Miklos Hamar, Matey Krastev, Kristiyan Hristov, David Beglou; <em>Explaining
Temporal Graph Models through an Explorer-Navigator Framework</em>,
<a href="https://openreview.net/forum?id=FI1XvwpchC" target=_blank rel=noopener>OpenReview</a></li><li>Clio Feng, Colin Bot, Bart den Boef, Bart Aaldering; <em>Reproducibility Study of
&ldquo;Explaining RL Decisions with Trajectories&rdquo;</em>,
<a href="https://openreview.net/forum?id=JQoWmeNaC2" target=_blank rel=noopener>OpenReview</a></li><li>Ethan Harvey, Mikhail Petrov, Michael C. Hughes; <em>Transfer Learning with
Informative Priors: Simple Baselines Better than Previously Reported</em>,
<a href="https://openreview.net/forum?id=BbvSU02jLg" target=_blank rel=noopener>OpenReview</a></li><li>Gijs de Jong,Macha Meijer,Derck W.E. Prinzhorn,Harold Ruiter; <em>Reproducibility
study of FairAC</em>, <a href="https://openreview.net/forum?id=ccDi5jtSF7" target=_blank rel=noopener>OpenReview</a></li><li>Nesta Midavaine, Gregory Hok Tjoan Go, Diego Canez, Ioana Simion, Satchit
Chatterji; <em>On the Reproducibility of Post-Hoc Concept Bottleneck Models</em>;
<a href="https://openreview.net/forum?id=8UfhCZjOV7" target=_blank rel=noopener>OpenReview</a></li><li>Jiapeng Fan, Paulius Skaigiris, Luke Cadigan, Sebastian Uriel Arias;
<em>Reproducibility Study of &ldquo;Learning Perturbations to Explain Time Series
Predictions&rdquo;</em>, <a href="https://openreview.net/forum?id=fCNqD2IuoD" target=_blank rel=noopener>OpenReview</a></li><li>Karim Ahmed Abdel Sadek, Matteo Nulli, Joan Velja, Jort Vincenti; <em>Explaining
RL Decisions with Trajectories’: A Reproducibility Study</em>,
<a href="https://openreview.net/forum?id=QdeBbK5CSh" target=_blank rel=noopener>OpenReview</a></li><li>Markus Semmler, Miguel de Benito Delgado; <em>Classwise-Shapley values for data
valuation</em> <a href="https://openreview.net/forum?id=srFEYJkqD7" target=_blank rel=noopener>OpenReview</a></li><li>Abdel Sadek Karim Ahmed, Nulli Matteo, Velja Joan, Vincenti Jort; <em>Explaining
RL Decisions with Trajectories: A Reproducibility Study</em>,
<a href="https://openreview.net/forum?id=QdeBbK5CSh" target=_blank rel=noopener>OpenReview</a></li><li>Daniel Gallo Fernández, Răzvan-Andrei Matișan, Alejandro Monroy Muñoz, Janusz
Partyka; <em>Reproducibility Study of &ldquo;ITI-GEN: Inclusive Text-to-Image
Generation&rdquo;</em> <a href="https://openreview.net/forum?id=d3Vj360Wi2" target=_blank rel=noopener>OpenReview</a></li><li>Kacper Bartosik, Eren Kocadag, Vincent Loos, Lucas Ponticelli;
<em>Reproducibility study of &ldquo;Robust Fair Clustering: A Novel Fairness Attack and
Defense Framework&rdquo;</em>, <a href="https://openreview.net/forum?id=Xu1sEPhjqH" target=_blank rel=noopener>OpenReview</a></li><li>Barath Chandran C; <em>CUDA: Curriculum of Data Augmentation for Long‐Tailed
Recognition</em>, <a href="https://openreview.net/forum?id=Wm6d44I8St" target=_blank rel=noopener>OpenReview</a></li><li>Kacper Bartosik, Eren Kocadag, Vincent Loos, Lucas Ponticelli;
<em>Reproducibility study of &ldquo;Robust Fair Clustering: A Novel Fairness Attack and
Defense Framework&rdquo;</em>, <a href="https://openreview.net/forum?id=Xu1sEPhjqH" target=_blank rel=noopener>OpenReview</a></li><li>Christina Isaicu, Jesse Wonnink, Andreas Berentzen, Helia Ghasemi;
<em>Reproducibility Study of “Explaining Temporal Graph Models Through an
Explorer-Navigator Framework"</em>,
<a href="https://openreview.net/forum?id=9M2XqvH2SB" target=_blank rel=noopener>OpenReview</a></li><li>Iason Skylitsis, Zheng Feng, Idries Nasim, Camille Niessink; <em>Reproducibility
Study of &ldquo;Robust Fair Clustering: A Novel Fairness Attack and Defense
Framework&rdquo;</em>, <a href="https://openreview.net/forum?id=H1hLNjwrGy" target=_blank rel=noopener>OpenReview</a></li><li>Fatemeh Nourilenjan Nokabadi, Jean-Francois Lalonde, Christian Gagné;
<em>Reproducibility Study on Adversarial Attacks Against Robust Transformer
Trackers</em>, <a href="https://openreview.net/forum?id=FEEKR0Vl9s" target=_blank rel=noopener>OpenReview</a></li><li>Luan Fletcher, Robert van der Klis, Martin Sedlacek, Stefan Vasilev, Christos
Athanasiadis; <em>Reproducibility study of “LICO: Explainable Models with
Language-Image Consistency"</em>,
<a href="https://openreview.net/forum?id=Mf1H8X5DVb" target=_blank rel=noopener>OpenReview</a></li><li>Wouter Bant, Ádám Divák, Jasper Eppink, Floris Six Dijkstra; <em>On the
Reproducibility of: &ldquo;Learning Perturbations to Explain Time Series
Predictions&rdquo;</em>, <a href="https://openreview.net/forum?id=nPZgtpfgIx" target=_blank rel=noopener>OpenReview</a></li><li>Berkay Chakar,Amina Izbassar,Mina Janićijević,Jakub Tomaszewski;
<em>Reproducibility Study: Equal Improvability: A New Fairness Notion Considering
the Long-Term Impact</em>,
<a href="https://openreview.net/forum?id=Yj8fUQGXXL" target=_blank rel=noopener>OpenReview</a></li><li>Oliver Bentham, Nathan Stringham, Ana Marasović; <em>Chain-of-Thought
Unfaithfulness as Disguised Accuracy</em>,
<a href="https://openreview.net/forum?id=ydcrP55u2e" target=_blank rel=noopener>OpenReview</a></li><li>Shivank Garg, Manyana Tiwari; <em>Unmasking the Veil: An Investigation into
Concept Ablation for Privacy and Copyright Protection in Images</em>
<a href="https://openreview.net/forum?id=TYYApLzjaQ" target=_blank rel=noopener>OpenReview</a></li></ul><p>If you are an author of the below mentioned papers and have not
<a href=https://forms.gle/JJ28rLwBSxMriyE89 target=_blank rel=noopener>submitted the form</a> with the camera ready
items, please consider doing so at the earliest. We will reach out to the
accepted authors soon with the next steps. We will also announce the best paper
awards and share details on the logistics of NeurIPS poster session in the
coming weeks.</p><h2 id=an-update-on-decisions>An update on decisions</h2><p><em>July 5th, 2024</em></p><p>We initially communicated to have all decisions of MLRC 2023 out by 31st of
May, 2024. Unfortunately, several submissions are still under review at TMLR,
and we are waiting for the final decisions to trickle in. Overall, MLRC 2023 had
46 valid submissions, out of which we have recieved decisions on 61% of them. We
are in touch with TMLR to expedite the process of decisions for the remaining
submissions - we expect all decisions to come in by the next couple of weeks.</p><p>Until then, we are happy to announce the (partial) list of accepted papers.
Congratulations to all 🎉! If you are an author of the below mentioned
papers and have not <a href=https://forms.gle/JJ28rLwBSxMriyE89 target=_blank rel=noopener>submitted the form</a>
with the camera ready items, please consider doing so at the earliest. We will
reach out to the accepted authors soon with the next steps.</p><p><em>(partial paper list removed as we release the final list above)</em></p><h2 id=deprecated-call-for-papers>[Deprecated] Call For Papers</h2><p>We invite contributions from academics, practitioners and industry researchers
of the ML community to submit novel and insightful reproducibility studies.
Please read our <a href=/blog/announcing_mlrc2023/>blog post</a> regarding our
retrospectives of running the challenge and the future roadmap. We are happy to
announce the formal partnership with
<a href=https://jmlr.org/tmlr/ target=_blank rel=noopener>Transactions of Machine Learning Research (TMLR)</a>
journal. The challenge goes live on <strong>October 23, 2023</strong>.</p><p>We recommend you choose any paper(s) published in the 2023 calendar year from
the top conferences and journals (<a href=https://neurips.cc/ target=_blank rel=noopener>NeurIPS</a>,
<a href=https://icml.cc/ target=_blank rel=noopener>ICML</a>, <a href=https://iclr.cc/ target=_blank rel=noopener>ICLR</a>,
<a href=https://2023.aclweb.org/ target=_blank rel=noopener>ACL</a>, <a href=https://2023.emnlp.org/ target=_blank rel=noopener>EMNLP</a>,
<a href=https://iccv2023.thecvf.com/ target=_blank rel=noopener>ICCV</a>,
<a href=https://cvpr2023.thecvf.com/Conferences/2023 target=_blank rel=noopener>CVPR</a>,
<a href=https://jmlr.org/tmlr/ target=_blank rel=noopener>TMLR</a>, <a href=https://jmlr.org/ target=_blank rel=noopener>JMLR</a>,
<a href=https://transacl.org/index.php/tacl target=_blank rel=noopener>TACL</a>) to run your reproducibility study
on.</p><figure class=mlrc_dark><div class="d-flex justify-content-center"><div class=w-100><img src=uploads/mlrc.drawio.svg alt loading=lazy data-zoomable></div></div></figure><figure class=mlrc_light><div class="d-flex justify-content-center"><div class=w-100><img src=uploads/mlrc.light.drawio.svg alt loading=lazy data-zoomable></div></div></figure><p>In order for your paper to be submitted and presented at MLRC 2023, it first
needs to be <strong>accepted and published</strong> at TMLR. While TMLR aims to follow a
2-months timeline to complete the review process of its regular submissions,
this timeline is not guaranteed. If you haven’t already, we therefore recommend
submitting your original paper to TMLR by <strong>February 16th, 2024</strong>, that is a
little over 3 months in advance of the MLRC publication announcement date.</p><h2 id=key-dates>Key Dates</h2><ul><li>Challenge goes live: October 23, 2023</li><li>Deadline to share your <strong>intent to submit</strong> a TMLR paper to MLRC: <strong>February
16th, 2024</strong> at the following form: <strong><a href=https://forms.gle/JJ28rLwBSxMriyE89 target=_blank rel=noopener>https://forms.gle/JJ28rLwBSxMriyE89</a></strong>.
This form requires that you provide a link to your TMLR submission. Once it
gets accepted (if it isn’t already), you should then update the same form with
your paper camera ready details. Your accepted TMLR paper will finally undergo
a light AC review to verify MLRC compatibility.</li><li>We aim to announce the accepted papers by <del><strong>May 31st, 2024</strong></del> <strong>July 17th,
2024</strong>, pending decisions of all papers.</li></ul><h2 id=contact-information>Contact Information</h2><ul><li>For query regarding MLRC 2023, mail us at:
<a href=mailto:mlrc-2023@googlegroups.com>mlrc-2023@googlegroups.com</a>.</li><li>For general queries, media, sponsorship, partnership requests, mail us at
<a href=reproducibility.challenge@gmail.com>reproducibility.challenge@gmail.com</a>.</li></ul></div><div class=article-widget><div class=post-nav></div></div></div><div class=body-footer><p>Last updated on Oct 22, 2023</p></div></article><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 ML Reproducibility Challenge.</p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.85290d887e7fcdd400ccb3ffb9bcd3e3.js></script></body></html>