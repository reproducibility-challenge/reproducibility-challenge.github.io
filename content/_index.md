---
title: ML Reproducibility Challenge
type: book # Do not modify.
toc: false
headless: true
---

Welcome to the ML Reproducibility Challenge 2023. This is the seventh edition of the event ([v1](https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html), [v2](https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html), [v3](https://reproducibility-challenge.github.io/neurips2019/), [v4](https://paperswithcode.com/rc2020), [v5](https://paperswithcode.com/rc2021), [v6](https://paperswithcode.com/rc2022))

The primary goal of this event is to encourage the publishing and sharing of scientific results that are reliable and reproducible. In support of this, the objective of this challenge is to investigate reproducibility of papers accepted for publication at top conferences by inviting members of the community at large to select a paper, and verify the empirical results and claims in the paper by reproducing the computational experiments, either via a new implementation or using code/data or other information provided by the authors.

## Retrospectives
Over the years, the ML Reproducibility Challenge has been largely aimed at students beginning their journey in ML research as part of their ML coursework. This provided an accessible entry point to ML research, allowing early career researchers to participate and learn a full paper publication lifecycle - from designing the research question to investigating the limits of a scientific hypothesis to final publication acceptance. One of the success stories of this approach was from University of Amsterdam, who designed a course around the challenge (FACT AI), and consistently produced high quality reproducibility reports in the final proceedings of the challenge.
While the challenge is a valuable resource to ML course instructors and early career ML researchers, we are eager for the challenge to grow in scope and impact. More specifically, we want to encourage ML researchers to contribute novel research that will improve scientific practice and understanding in the field. We thus identified several shortcomings of the current model following our retrospection of the submitted and accepted papers in the challenge.



## News

- [MM/DD/YYYY] A thing happened
- [MM/DD/YYYY] Another thing happened

## Key Dates

- Announcement of the challenge: MM/DD/YYYY
- Submission deadline: MM/DD/YYYY (11:59PM AOE), platform: TBA

## Invitation to Participate

Come join

## How to participate
We invite contributions from academics, practitioners and industry researchers of the ML community to submit novel and insightful reproducibility studies.

In order for your paper to be submitted and presented at MLRC 2023, it first needs to be submitted and accepted and published at TMLR. While TMLR aims to follow a **2-months** timeline to complete the review process of its regular submissions, this timeline is not guaranteed. We therefore recommend submitting your paper to TMLR at least 3 months in advance of the MLRC deadline. 
Challenge goes live: October 17, 2023
You can choose any paper(s) published in the 2023 calendar year from the top conferences and journals (NeurIPS, ICML, ICLR, ACL, EMNLP, ECCV, CVPR, TMLR, JMLR, TACL) to run your reproducibility study on.
Submit your paper to [TMLR](https://openreview.net/group?id=TMLR).

**Recommended deadline** to submit your paper to TMLR: **February 1st, 2024**

Submit information about your accepted and published TMLR paper to this [form](https://forms.gle/JJ28rLwBSxMriyE89). Your accepted TMLR paper will then undergo a light final AC review to verify MLRC compatibility.

**Deadline** to submit your TMLR accepted papers in the above form: **May 1st, 2024, EOD (Anywhere on Earth)**


## Contact Information
For general queries regarding the challenge, mail us at reproducibility.challenge@gmail.com.

## Organizing Committee
- Koustuv Sinha, Meta (FAIR)
- Jessica Zosa Forde, Brown University
- Mandana Samiei, McGill University, Mila
- Arna Ghosh, McGill University, Mila
- Lintang Sutawika, Eleuther AI

## Reproducible ML Advisory Board
- Joelle Pineau, Meta (FAIR), McGill University, Mila
- Hugo Larochelle, Google Deepmind, Université de Montréal, Mila
- Robert Stojnic, Meta 
- D. Sculley, Kaggle

## Acknowledgements
- Reviewers
- Organizers
- PapersWithCode
- Kaggle
- OpenReview

