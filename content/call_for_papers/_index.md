---
# Title, summary, and page position.
linktitle: Call for Papers
weight: 20
icon: task-square-svgrepo-com
icon_pack: fas

# Page metadata.
title: Call for Papers
date: "2024-12-12T00:00:00Z"
type: book # Do not modify.
---

<!-- Call for papers announcement coming soon! -->

The Machine Learning Reproducibility Challenge (MLRC 2025) encourages the
community to investigate the reproducibility, replicability and generalisability
of published claims in top conferences in the literature.

Submissions must be first accepted at [TMLR](https://jmlr.org/tmlr/) to be
considered in the MLRC 2025 Proceedings. Please read the
[author guidelines](https://jmlr.org/tmlr/author-guide.html) and
[submission guidelines](https://jmlr.org/tmlr/editorial-policies.html) from TMLR
to get the submission format and to understand more about the reviewing process.
Existing papers related to the scope (with reproducibility certification)
already published at TMLR are also welcome for the consideration of the
committee. Please read our [announcement blog post](/blog/announcing_mlrc2025/)
for more information.

{{< figure src="../../uploads/mlrc2025.drawio.svg" class="mlrc_dark" >}}

{{< figure src="../../uploads/mlrc2025.light.drawio.svg" class="mlrc_light" >}}

## Scope

We invite submissions which conduct novel, unpublished research of
reproducibility of machine learning methods and literature, including but not
limited to :

- Methods and tools to foster reproducibility research in Machine Learning
- Generalisability of published claims: novel insights and results beyond what
  was presented in the original paper, from any paper (or set of papers)
  published in top ML conferences and journals.
- Meta-reproducibility studies on a set of related papers.
- Meta analysis on the state of reproducibility in various subfields in Machine
  Learning.

## Important Dates

- Submit to TMLR OpenReview: https://openreview.net/group?id=TMLR
- Deadline to share your intent to submit a TMLR paper to MLRC: **February 21st,
  2025**
- This form requires that you provide a link to your TMLR submission. Once it
  gets accepted (if it isn’t already), you should then update the same form with
  your paper camera ready details.
- Cutoff deadline for receiving TMLR decisions: **June 20th, 2025**
- Deadline for announcing accepted papers: **June 27th, 2025**
- Conference day: **August 21st, 2025** at Princeton University, NJ, USA

## Camera Ready Process

- After you have updated the form with your accepted TMLR paper, it will finally
  undergo a light AC review to verify MLRC compatibility.
- AC review will determine the submissions eligible for talks and posters, which
  will be communicated alongside the acceptance decision.
- Post acceptance, we will reach out to you for logistics of submitting your
  videos and posters for the conference.

For query regarding MLRC 2025, contact us at
[mlrc-2025@googlegroups.com](mailto:mlrc-2025@googlegroups.com).

Koustuv Sinha

General Chair, MLRC 2025

<!-- ## Task Scope

We recommend you focus on the central claim of the paper. For example, if a paper introduces a new RL learning algorithm that performs better in sparse-reward environments, verify that you can re-implement the algorithm, run it on the same benchmarks and get results that are close to those in the original paper (exact reproducibility is in most cases very difficult due to minor implementation details). You do not need to reproduce all experiments in your selected paper, but only those that you feel are sufficient for you to verify the validity of the central claim.

If available, the authors’ code can and should be used; authors increasingly release their code and this is increasingly seen as an integral part of the publication process. Just re-running code is not a reproducibility study, and you need to approach any code with critical thinking and verify it does what is described in the paper and that these are sufficient to support the conclusions of the papers. Consider designing and running unit tests on the code to verify it works well and as described. Alternately, the methods presented can also be fully re-implemented according to the description in the paper. This is a higher bar for reproducibility that can take much more time, but may be helpful in detecting anomalies in the code, or shedding light on aspects of the implementation that affect results. In the end, what you choose to do will depend on your resources and how confident you want to be about the central claim of the paper.

Generally, a report should include any information future researchers or practitioners would find useful for reproducing or building upon the chosen paper. The results of any experiments should be included; a “negative result” which doesn’t support the main claims of the original paper is still valuable.

We also strongly encourage you to get in touch with the original authors to seek clarification and make sure your reproducibility report fairly reflects on their research and work with them to improve it.

## Proposed Outcomes

The goal of this challenge is not to criticize papers or the hard work of our fellow researchers. Science is not a competitive sport. Thus, the main objective of this challenge is to enable a mutually beneficial learning experience, while contributing to the research by strengthening the quality of the original paper.

Participants should produce a Reproducibility report, describing the target questions, experimental methodology, implementation details, analysis and discussion of findings, conclusions on reproducibility of the paper. This report should be posted as a contributed review on OpenReview.

The result of the reproducibility study should NOT be a simple Pass / Fail outcome. The goal should be to identify which parts of the contribution can be reproduced, and at what cost in terms of resources (computation, time, people, development effort, communication with the authors).

Participants should expect to engage in dialogue with original paper authors through the OpenReview site. Reproducibility Reports will be published at ReScience journal after peer review through OpenReview. -->
