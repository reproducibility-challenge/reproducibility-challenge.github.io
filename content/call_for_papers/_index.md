---
# Title, summary, and page position.
linktitle: Call for Papers
weight: 20
icon: task-square-svgrepo-com
icon_pack: fas

# Page metadata.
title: Call for Papers
date: "2023-10-22T00:00:00Z"
type: book # Do not modify.
---

The Machine Learning Reproducibility Challenge (MLRC 2023) is an unique, online
conference which encourages the community to investigate the reproducibility,
replicability and generalisability of published claims in top conferences in the
literature. We invite submissions which investigate the recently published
claims, add novel insights to them, and enable reproducible research, spanning
various topics in the ML literature. Submissions must be first accepted at
[TMLR](https://jmlr.org/tmlr/) to be considered in the MLRC 2023 Proceedings.
Please read the [author guidelines](https://jmlr.org/tmlr/author-guide.html) and
[submission guidelines](https://jmlr.org/tmlr/editorial-policies.html) from TMLR
to get the submission format and to understand more about the reviewing process.
Please read our [announcement blog post](/blog/announcing_mlrc2023/) for more
motivation, retrospectives and roadmap for the challenge.

## Scope

We invite thorough reproducibility studies, including but not limited to:

- _Generalisability_ of published claims: novel insights and results beyond what
  was presented in the original paper. We recommend you choose any paper(s)
  published in the 2023 calendar year from the top conferences and journals
  ([NeurIPS](https://neurips.cc/), [ICML](https://icml.cc/),
  [ICLR](https://iclr.cc/), [ACL](https://2023.aclweb.org/),
  [EMNLP](https://2023.emnlp.org/), [ICCV](https://iccv2023.thecvf.com/),
  [CVPR](https://cvpr2023.thecvf.com/Conferences/2023),
  [TMLR](https://jmlr.org/tmlr/), [JMLR](https://jmlr.org/),
  [TACL](https://transacl.org/index.php/tacl)) to run your reproducibility study
  on.
- Meta-reproducibility studies on set of related papers.
- Research on tools enabling reproducible research.
- Meta analysis on the state of reproducibility in various subfields in Machine
  Learning.

## Important Dates

- Challenge goes live: October 23, 2023
- Submit to TMLR OpenReview: https://openreview.net/group?id=TMLR
- Deadline to share your **intent to submit** a TMLR paper to MLRC: **February
  16th, 2024** at the following form: **https://forms.gle/JJ28rLwBSxMriyE89**.
  This form requires that you provide a link to your TMLR submission. Once it
  gets accepted (if it isn’t already), you should then update the same form with
  your paper camera ready details.
- We aim to announce the accepted papers by **May 31st, 2024**, pending
  decisions of all papers.

## Camera Ready Process

- After you have updated the form with your accepted TMLR paper, it will finally
  undergo a light AC review to verify MLRC compatibility.
- We will publish a proceedings booklet post announcement of all decisions.
- Accepted papers will be featured in our website along with 5-min companion
  videos.

For query regarding MLRC 2023, contact us at
[mlrc-2023@googlegroups.com](mailto:mlrc-2023@googlegroups.com).

Koustuv Sinha

Program Chair, MLRC 2023

<!-- ## Task Scope

We recommend you focus on the central claim of the paper. For example, if a paper introduces a new RL learning algorithm that performs better in sparse-reward environments, verify that you can re-implement the algorithm, run it on the same benchmarks and get results that are close to those in the original paper (exact reproducibility is in most cases very difficult due to minor implementation details). You do not need to reproduce all experiments in your selected paper, but only those that you feel are sufficient for you to verify the validity of the central claim.

If available, the authors’ code can and should be used; authors increasingly release their code and this is increasingly seen as an integral part of the publication process. Just re-running code is not a reproducibility study, and you need to approach any code with critical thinking and verify it does what is described in the paper and that these are sufficient to support the conclusions of the papers. Consider designing and running unit tests on the code to verify it works well and as described. Alternately, the methods presented can also be fully re-implemented according to the description in the paper. This is a higher bar for reproducibility that can take much more time, but may be helpful in detecting anomalies in the code, or shedding light on aspects of the implementation that affect results. In the end, what you choose to do will depend on your resources and how confident you want to be about the central claim of the paper.

Generally, a report should include any information future researchers or practitioners would find useful for reproducing or building upon the chosen paper. The results of any experiments should be included; a “negative result” which doesn’t support the main claims of the original paper is still valuable.

We also strongly encourage you to get in touch with the original authors to seek clarification and make sure your reproducibility report fairly reflects on their research and work with them to improve it.

## Proposed Outcomes

The goal of this challenge is not to criticize papers or the hard work of our fellow researchers. Science is not a competitive sport. Thus, the main objective of this challenge is to enable a mutually beneficial learning experience, while contributing to the research by strengthening the quality of the original paper.

Participants should produce a Reproducibility report, describing the target questions, experimental methodology, implementation details, analysis and discussion of findings, conclusions on reproducibility of the paper. This report should be posted as a contributed review on OpenReview.

The result of the reproducibility study should NOT be a simple Pass / Fail outcome. The goal should be to identify which parts of the contribution can be reproduced, and at what cost in terms of resources (computation, time, people, development effort, communication with the authors).

Participants should expect to engage in dialogue with original paper authors through the OpenReview site. Reproducibility Reports will be published at ReScience journal after peer review through OpenReview. -->
